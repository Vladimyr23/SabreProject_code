{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5febeeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#Environment set-up and libraries\n",
    "\n",
    "#Base libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "#Plotting libraries\n",
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#Utilities libraries\n",
    "from glob import glob \n",
    "import os\n",
    "\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588aaf02",
   "metadata": {},
   "source": [
    "### Loading the point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3a49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    print(file_name)\n",
    "\n",
    "    if file_name.endswith(\".las\") or file_name.endswith(\".laz\"):\n",
    "        print(\"[INFO] .las (.laz) file loading\")\n",
    "        try:\n",
    "            # import lidar .las data and assign to variable\n",
    "            pcd = laspy.read(file_name)\n",
    "            # examine the available features for the lidar file we have read\n",
    "            # list(las.point_format.dimension_names)\n",
    "            #\n",
    "            # set(list(las.classification))\n",
    "\n",
    "            # Creating, Filtering, and Writing Point Cloud Data\n",
    "            # To create 3D point cloud data, we can stack together with the X, Y, and Z dimensions, using Numpy like this.\n",
    "            point_data = np.stack([pcd.X, pcd.Y, pcd.Z], axis=0).transpose((1, 0))\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_data)\n",
    "            # points = point_data\n",
    "            if pcd is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "\n",
    "                # Point cloud\n",
    "                return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".las, .laz file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".e57\"):\n",
    "        print(\"[INFO] .e57 file loading\")\n",
    "        try:\n",
    "            e57_file = pye57.E57(file_name)\n",
    "\n",
    "            # other attributes can be read using:\n",
    "            data = e57_file.read_scan(0)\n",
    "\n",
    "            # 'data' is a dictionary with the point types as keys\n",
    "            # assert isinstance(data[\"cartesianX\"], np.ndarray)\n",
    "            # assert isinstance(data[\"cartesianY\"], np.ndarray)\n",
    "            # assert isinstance(data[\"cartesianZ\"], np.ndarray)\n",
    "\n",
    "            point_xyz = np.stack([data[\"cartesianX\"], data[\"cartesianY\"], data[\"cartesianZ\"]]).transpose((1, 0))\n",
    "            # points_rgb = [data[\"colorRed\"], data[\"colorGreen\"], data[\"colorBlue\"]]\n",
    "            # points_intensity = data[\"intensity\"]\n",
    "\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            # points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            # points = point_xyz\n",
    "            # pcd.colors = o3d.utility.Vector3dVector(points_rgb)\n",
    "            # pcd.colors[:, 0] = points_intensity\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".e57 file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".bin\"):\n",
    "        print(\"[INFO] .bin file loading\")\n",
    "        try:\n",
    "            size_float = 4\n",
    "            list_pcd = []\n",
    "            with open(file_name, \"rb\") as f:\n",
    "                byte = f.read(size_float * 4)\n",
    "                while byte:\n",
    "                    x, y, z, intensity = struct.unpack(\"ffff\", byte)\n",
    "                    list_pcd.append([x, y, z])\n",
    "                    byte = f.read(size_float * 4)\n",
    "            np_pcd = np.asarray(list_pcd)\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(np_pcd)\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".bin file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".ply\"):\n",
    "        pcd = o3d.io.read_point_cloud(file_name)\n",
    "        points_xyz = np.asarray(pcd.points)\n",
    "        #pcd = o3d.geometry.PointCloud() # No need to do that already a PointCloud\n",
    "        pcd.points = o3d.utility.Vector3dVector(points_xyz)\n",
    "        # points = points_xyz\n",
    "        if pcd is not None:\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            # Point cloud\n",
    "            return pcd\n",
    "\n",
    "    elif file_name.endswith(\".pts\"):\n",
    "        try:\n",
    "            with open(file_name, \"r\") as f:\n",
    "                # Log every 1000000 lines.\n",
    "                LOG_EVERY_N = 1000000\n",
    "                points_np = []\n",
    "                for line in f:\n",
    "                    if len(line.split()) == 4:\n",
    "                        x, y, z, i = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z), float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 3:\n",
    "                        x, y, z = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 5:\n",
    "                        x, y, z, i, zeroes_v = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z), float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 7:\n",
    "                        x, y, z, r, g, b, i = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z),\n",
    "                                          float(r), float(g), float(b),\n",
    "                                          float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    else:\n",
    "                        print(\"[Info] The file has unregistered format\")\n",
    "                        return\n",
    "            print('loop end')\n",
    "            points_arr = np.array(points_np).transpose()\n",
    "            print(len(points_arr))\n",
    "            point_xyz = points_arr[:3].transpose()\n",
    "            print(\"xyz points shape\", point_xyz.shape)\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            if len(points_arr) == 4:\n",
    "                points_intensity = (points_arr[3])/255.0\n",
    "                print(\"intensity points len\", points_intensity.shape)\n",
    "                points_intensity_rgb = np.vstack((points_intensity,\n",
    "                                                  points_intensity,\n",
    "                                                  points_intensity)).T\n",
    "                print(\"intensity_rgb points shape\", points_intensity_rgb.shape)\n",
    "                pcd.colors = o3d.utility.Vector3dVector(points_intensity_rgb)\n",
    "            elif len(points_arr) == 7:\n",
    "                points_red = (points_arr[4]) / 255.0\n",
    "                points_green = (points_arr[5]) / 255.0\n",
    "                points_blue = (points_arr[6]) / 255.0\n",
    "                points_rgb = np.vstack((points_red,\n",
    "                                        points_green,\n",
    "                                        points_blue)).T\n",
    "\n",
    "                # points_intensity = ((points_arr[3]) / 255.0).T\n",
    "                # print(\"intensity points len\", points_intensity.shape)\n",
    "                print(\"rgb points shape\", points_rgb.shape)\n",
    "                pcd.colors = o3d.utility.Vector3dVector(points_rgb)\n",
    "                #pcd.intensities = o3d.utility.Vector3dVector(points_intensity)\n",
    "            if pcd is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "                # Point cloud\n",
    "                return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\"[Info] Reading .pts file failed\", file_name)\n",
    "\n",
    "    # elif file_name.endswith(\".kml\"):\n",
    "    #     try:\n",
    "    #         with open(file_name, \"r\") as f:\n",
    "    #             # Log every 1000000 lines.\n",
    "    #             LOG_EVERY_N = 1000000\n",
    "    #             points_np = []\n",
    "    #             for line in f:\n",
    "    #                 print(line)\n",
    "    #                 if len(line.split(\",\")) == 3 and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "    #                     y, x, z = [num for num in line.split(\",\")]\n",
    "    #                     points_np.append([float(x), float(y), float(z)])\n",
    "    #                     if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "    #                         print('point', len(points_np))\n",
    "    #                 else:\n",
    "    #                     print(\"[Info] The file has unregistered format\")\n",
    "    #         print('loop end')\n",
    "    #         points_arr = np.array(points_np).transpose()\n",
    "    #         print(len(points_arr))\n",
    "    #         point_xyz = points_arr[:3].transpose()\n",
    "    #         # points_intensity = points_arr[3]\n",
    "    #         pcd = o3d.geometry.PointCloud()\n",
    "    #         pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "    #         if pcd is not None:\n",
    "    #             print(\"[Info] Successfully read\", file_name)\n",
    "    #             # Point cloud\n",
    "    #             return pcd\n",
    "    #\n",
    "    #     except Exception:\n",
    "    #         print(\"[Info] Reading .kml file failed\", file_name)\n",
    "\n",
    "    else:\n",
    "        pcd = None\n",
    "        geometry_type = o3d.io.read_file_geometry_type(file_name)\n",
    "        print(geometry_type)\n",
    "\n",
    "        mesh = None\n",
    "        if geometry_type & o3d.io.CONTAINS_TRIANGLES:\n",
    "            mesh = o3d.io.read_triangle_model(file_name)\n",
    "        if mesh is None:\n",
    "            print(\"[Info]\", file_name, \"appears to be a point cloud\")\n",
    "            cloud = None\n",
    "            try:\n",
    "                cloud = o3d.io.read_point_cloud(file_name)\n",
    "                # print(type(cloud))\n",
    "            except Exception:\n",
    "                print(\"[Info] Unknown filename\", file_name)\n",
    "            if cloud is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "\n",
    "                if not cloud.has_normals():\n",
    "                    cloud.estimate_normals()\n",
    "                cloud.normalize_normals()\n",
    "                pcd = cloud\n",
    "                #points = cloud.points\n",
    "                pcd.points = o3d.utility.Vector3dVector(cloud.points)\n",
    "            else:\n",
    "                print(\"[WARNING] Failed to read points\", file_name)\n",
    "\n",
    "        if pcd is not None or mesh is not None:\n",
    "            try:\n",
    "                if mesh is not None:\n",
    "                    # Triangle model\n",
    "                    _scene.scene.add_model(\"__model__\", mesh)\n",
    "                else:\n",
    "                    # Point cloud\n",
    "                    return pcd\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a183d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/SABRE MMS_S3 - 0002.pts\n",
      "point 1000000\n",
      "point 2000000\n",
      "point 3000000\n",
      "point 4000000\n",
      "point 5000000\n",
      "point 6000000\n",
      "point 7000000\n",
      "point 8000000\n",
      "point 9000000\n",
      "point 10000000\n",
      "point 11000000\n",
      "loop end\n",
      "4\n",
      "xyz points shape (11008825, 3)\n",
      "intensity points len (11008825,)\n",
      "intensity_rgb points shape (11008825, 3)\n",
      "[Info] Successfully read /home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/SABRE MMS_S3 - 0002.pts\n",
      "/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/SABRE Static Scan_T17_003.pts\n",
      "point 1000000\n",
      "point 2000000\n",
      "point 3000000\n",
      "point 4000000\n",
      "point 5000000\n",
      "point 6000000\n",
      "point 7000000\n",
      "point 8000000\n",
      "point 9000000\n",
      "point 10000000\n",
      "point 11000000\n",
      "point 12000000\n",
      "point 13000000\n",
      "point 14000000\n",
      "point 15000000\n",
      "point 16000000\n",
      "point 17000000\n",
      "loop end\n",
      "4\n",
      "xyz points shape (17814760, 3)\n",
      "intensity points len (17814760,)\n",
      "intensity_rgb points shape (17814760, 3)\n",
      "[Info] Successfully read /home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/SABRE Static Scan_T17_003.pts\n"
     ]
    }
   ],
   "source": [
    "filepath_mob1 = \"/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/\"\n",
    "filename1 = \"SABRE MMS_S3 - 0002.pts\"\n",
    "filepath1 = filepath_mob1 + filename1\n",
    "pc1 = load_file(filepath1)\n",
    "\n",
    "filepath_static2 = \"/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/\"\n",
    "filename2 = \"SABRE Static Scan_T17_003.pts\"\n",
    "filepath2 = filepath_static2 + filename2\n",
    "pc2 = load_file(filepath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5f257",
   "metadata": {},
   "source": [
    "#### pc1 and pc2 are the original size point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a37495",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function that returns the down sampled point cloud \n",
    "and fpfh parameters of the down sampled point cloud\n",
    "the down sampling defined by the parameter voxel_size\n",
    "'''\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0,\n",
    "                                             max_nn=30))\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5.0,\n",
    "                                             max_nn=100))\n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35035bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "voxel_size = 0.3\n",
    "\n",
    "pc1_down, pc1_fpfh = preprocess_point_cloud(pc1, voxel_size)\n",
    "pc2_down, pc2_fpfh = preprocess_point_cloud(pc2, voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023554eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56746, 3)\n",
      "(123234, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sizes of the downsampled point clouds points sets\n",
    "pc1_down_points = np.asarray(pc1_down.points)\n",
    "pc2_down_points = np.asarray(pc2_down.points)\n",
    "print(pc1_down_points.shape)\n",
    "print(pc2_down_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf6808",
   "metadata": {},
   "source": [
    "#### Defining device for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425cc5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d52b95",
   "metadata": {},
   "source": [
    "### Initial RANSAC alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d6adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_size =  0.3\n",
      "Distance threshold:  0.75\n",
      "mutual_filter =  True\n",
      "max_iterations =  1000000\n",
      "max_validation =  28373\n",
      "\n",
      "RANSAC Started 10/23/2023, 10:29:21 \n",
      "\n",
      "Running RANSAC\n",
      "\n",
      "RANSAC Finished 10/23/2023, 10:29:25 \n",
      "Global registration took 4.869 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"voxel_size = \", voxel_size)\n",
    "distance_threshold = 2.5 * voxel_size\n",
    "print(\"Distance threshold: \", distance_threshold)\n",
    "mutual_filter = True\n",
    "print(\"mutual_filter = \", mutual_filter)\n",
    "max_iterations = 1000000\n",
    "print(\"max_iterations = \", max_iterations)\n",
    "max_validation = np.min([len(pc1_down.points), len(pc2_down.points)]) // 2\n",
    "print(\"max_validation = \", max_validation)\n",
    "\n",
    "# getting the current date and time\n",
    "start = datetime.now()\n",
    "# getting the date and time from the current date and time in the given format\n",
    "start_date_time = start.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print('\\nRANSAC Started', start_date_time, '\\n')\n",
    "print('Running RANSAC\\n')\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    pc1_down, pc2_down, pc1_fpfh, pc2_fpfh,\n",
    "    mutual_filter=mutual_filter,\n",
    "    max_correspondence_distance=distance_threshold,\n",
    "    estimation_method=o3d.pipelines.registration.\n",
    "    TransformationEstimationPointToPoint(True),\n",
    "    ransac_n=3,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(\n",
    "        max_iterations, max_validation))  # max_validation replaces args.confidence in mobile-static\n",
    "# getting the current date and time\n",
    "finish = datetime.now()\n",
    "# getting the date and time from the current date and time in the given format\n",
    "finish_date_time = finish.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print('RANSAC Finished', finish_date_time,\n",
    "      \"\\nGlobal registration took %.3f sec.\\n\" % (finish - start).total_seconds())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabecc9b",
   "metadata": {},
   "source": [
    "#### RANSAC transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b81080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated transformation matrix:\n",
      "[[-9.61128621e-01 -1.34747476e-01  8.03138338e-03  1.73479135e+06]\n",
      " [ 1.34977219e-01 -9.60040389e-01  4.57516409e-02  6.79695670e+06]\n",
      " [ 1.59241253e-03  4.64239161e-02  9.69449274e-01 -2.94502707e+05]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Saving the transformation matrix in ransac_transformation_matrix.txt ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = result.transformation\n",
    "print(\"The estimated transformation matrix:\")\n",
    "print(trans)\n",
    "print(\"Saving the transformation matrix in ransac_transformation_matrix.txt ...\")\n",
    "np.savetxt('ransac_transformation_matrix.txt', trans)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f538f8",
   "metadata": {},
   "source": [
    "#### Applying RANSAC transformation on original and downsampled point clouds and visualising the result with original point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d715a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_down_ransac = pc1_down.transform(result.transformation)\n",
    "pc1_ransac = pc1.transform(result.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "918cb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56746, 3)\n",
      "(123234, 3)\n"
     ]
    }
   ],
   "source": [
    "pc1_down_ransac_points = np.asarray(pc1_down_ransac.points)\n",
    "print(pc1_down_ransac_points.shape)\n",
    "print(pc2_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5030d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 17814760 points."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coloring the point clouds\n",
    "source_color=(1, 0.706, 0)\n",
    "target_color=(0, 0.651, 0.929)\n",
    "pc1_ransac.paint_uniform_color(source_color)\n",
    "pc2.paint_uniform_color(target_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0296a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pc1_ransac, pc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185bfb3",
   "metadata": {},
   "source": [
    "#### Cropping downsampled point cloud 2 (static scan) to the size of downsampled and transformed point cloud 1 (mobile scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b9fb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop point cloud 2 to the size of transformed point cloud 1\n",
    "oriented_bounding_box = pc1_down_ransac.get_oriented_bounding_box()\n",
    "oriented_bounding_box.color = (0, 1, 0)\n",
    "pc2_down_croppped = pc2_down.crop(oriented_bounding_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d138e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 88128 points."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coloring the downsampled, traansformed point cloud 1 \n",
    "# and the cropped point cloud 2\n",
    "pc1_down_ransac.paint_uniform_color(source_color)\n",
    "pc2_down_croppped.paint_uniform_color(target_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cefd2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both point clouds with visualization of the bbox\n",
    "o3d.visualization.draw_geometries([pc1_down_ransac, pc2_down_croppped, oriented_bounding_box, pc1_down_ransac])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfc3ea",
   "metadata": {},
   "source": [
    "### Chamfer distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46dee026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chamfer_distance(pcd1, pcd2):\n",
    "    \"\"\"\n",
    "    Compute the Chamfer distance between two point clouds.\n",
    "\n",
    "    Parameters:\n",
    "    - pcd1, pcd2: Open3D point cloud objects.\n",
    "\n",
    "    Returns:\n",
    "    - chamfer_distance: The Chamfer distance between the two point clouds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute distance from pcd1 to pcd2\n",
    "    distances_1_to_2 = pcd1.compute_point_cloud_distance(pcd2)\n",
    "    avg_distance_1_to_2 = np.mean([np.min(dist) for dist in distances_1_to_2])\n",
    "\n",
    "    # Compute distance from pcd2 to pcd1\n",
    "    distances_2_to_1 = pcd2.compute_point_cloud_distance(pcd1)\n",
    "    avg_distance_2_to_1 = np.mean([np.min(dist) for dist in distances_2_to_1])\n",
    "\n",
    "    # Compute the Chamfer distance\n",
    "    chamfer_distance = (avg_distance_1_to_2 + avg_distance_2_to_1) / 2\n",
    "\n",
    "    return chamfer_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b2d5475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer Distance: 2.518630661043157\n"
     ]
    }
   ],
   "source": [
    "chamfer_dist = compute_chamfer_distance(pc1_down_ransac, pc2_down_croppped)\n",
    "print(f\"Chamfer Distance: {chamfer_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b8d50",
   "metadata": {},
   "source": [
    "#### RANSAC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a08d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness:\n",
      "0.7057589962288091\n",
      "\n",
      "RMSE of all inlier correspondences:\n",
      "0.3158831683339917\n",
      "\n",
      "Correspondence Set:\n",
      "std::vector<Eigen::Vector2i> with 40049 elements.\n",
      "Use numpy.asarray() to access data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RANSAC Evaluation\n",
    "\n",
    "fitness = result.fitness\n",
    "print(\"Fitness:\")\n",
    "print(fitness)\n",
    "print(\"\")\n",
    "\n",
    "rmse = result.inlier_rmse\n",
    "print(\"RMSE of all inlier correspondences:\")\n",
    "print(rmse)\n",
    "print(\"\")\n",
    "\n",
    "# trans = result.transformation\n",
    "# print(\"The estimated transformation matrix:\")\n",
    "# print(trans)\n",
    "# print(\"Saving the transformation matrix in ransac_transformation_matrix.txt ...\")\n",
    "# np.savetxt('ransac_transformation_matrix.txt', trans)\n",
    "# print(\"\")\n",
    "\n",
    "correspondences = result.correspondence_set\n",
    "print(\"Correspondence Set:\")\n",
    "print(correspondences)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a69a0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def registration_error(sour, targ):\n",
    "    # # Make source and target of the same size\n",
    "    # minimum_len = min(len(sour), len(targ))\n",
    "    # source = sour[:minimum_len, :3]\n",
    "    # target = sour[:minimum_len, :3]\n",
    "    # # Apply transformation to point cloud\n",
    "    # source_transformed = np.dot(transformation[:3, :3], source.T).T + transformation[:3, 3]\n",
    "    # # Compute the difference between the transformed source and target point clouds\n",
    "    # diff = np.subtract(target, source_transformed)\n",
    "    # # RMSE of the difference\n",
    "    # rmse = np.sqrt(np.mean(np.sum(diff ** 2, axis=1)))\n",
    "    # # Compute the rotational error using quaternions\n",
    "    # r = R.from_matrix(transformation)\n",
    "    # q = r.as_quat()\n",
    "    # q_target = R.from_matrix(np.identity(3)).as_quat()\n",
    "    # rot_error = np.arccos(np.abs(np.dot(q, q_target))) * 180 / np.pi\n",
    "    # # Compute the translational error\n",
    "    # trans_error = np.linalg.norm(transformation - np.array([0, 0, 0]))\n",
    "    # return rmse, rot_error, trans_error\n",
    "    print('Calculating errors...')\n",
    "    # Calculate the centroid of the source and target points\n",
    "    source_centroid = np.mean(sour, axis=0)\n",
    "    target_centroid = np.mean(targ, axis=0)\n",
    "    print(f'Sour centroid: {source_centroid}')\n",
    "    print(f'Targ centroid: {target_centroid}')\n",
    "\n",
    "    # Calculate the covariance matrix of the source and target points\n",
    "    source_covariance = np.cov(sour.T)\n",
    "    target_covariance = np.cov(targ.T)\n",
    "\n",
    "    # Calculate the singular value decomposition of the covariance matrices\n",
    "    U_source, S_source, Vt_source = np.linalg.svd(source_covariance)\n",
    "    U_target, S_target, Vt_target = np.linalg.svd(target_covariance)\n",
    "\n",
    "    # Calculate the rotation matrix\n",
    "    rot = Vt_target.T @ U_source.T\n",
    "\n",
    "    # Calculate the translation vector\n",
    "    transl = target_centroid - rot @ source_centroid\n",
    "    print(f'Transl vector: {transl}')\n",
    "\n",
    "    rot_err = rot - np.eye(3)\n",
    "    # Mean Absolute error for each axis (row in rot_err)\n",
    "    rot_mae_xyz = np.mean(np.abs(rot_err), axis=1)\n",
    "\n",
    "    # Calculating translational error\n",
    "    transl_xyz = np.divide(np.abs(transl), (np.abs(source_centroid)+np.abs(target_centroid)+np.abs(transl))/3)\n",
    "    transl_xyz_mae = np.divide(transl_xyz, 100)\n",
    "    # Calculate the mean squared error\n",
    "    #mse = np.mean(np.sum((targ - (sour @ rot.T + transl)) ** 2, axis=1))\n",
    "\n",
    "    return rot_mae_xyz, transl_xyz_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27526fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating errors...\n",
      "Sour centroid: [3.71370502e+05 7.96958317e+05 6.60483966e+01]\n",
      "Targ centroid: [3.71370216e+05 7.96933154e+05 7.02526278e+01]\n",
      "Transl vector: [-299824.58809461  229044.24017572    7707.697467  ]\n",
      "Rotational MAE error xyz: [0.18740532 0.19354766 0.04054352], Translational MAE error xyz: [0.00862751 0.00376937 0.02947871]\n",
      "Rotational MAE: 0.1404988334065751, Translational MAE: 0.013958528487968835\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We have pc1_down_ransac_points need pc2_down_croppped_points\n",
    "pc2_down_croppped_points = np.asarray(pc2_down_croppped.points)\n",
    "\n",
    "rot_err, transl_err = registration_error(pc1_down_ransac_points, pc2_down_croppped_points)\n",
    "print(f'Rotational MAE error xyz: {rot_err}, Translational MAE error xyz: {transl_err}')\n",
    "print(f'Rotational MAE: {np.mean(rot_err)}, Translational MAE: {np.mean(transl_err)}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8afa1",
   "metadata": {},
   "source": [
    "## TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429799b8",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc576167",
   "metadata": {},
   "source": [
    "#### Downsampling again to fit the memory with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd69139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess point cloud data. One more downsampling\n",
    "voxel_size = 0.8  # Adjust as needed\n",
    "source_pc_down, source_fpfh = preprocess_point_cloud(pc1_down_ransac, voxel_size)\n",
    "target_pc_down, target_fpfh = preprocess_point_cloud(pc2_down_croppped, voxel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f56dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8886, 3)\n",
      "(16880, 3)\n"
     ]
    }
   ],
   "source": [
    "# Downsampling result\n",
    "source_pc_down_points = np.asarray(source_pc_down.points)\n",
    "target_pc_down_points = np.asarray(target_pc_down.points)\n",
    "print(source_pc_down_points.shape)\n",
    "print(target_pc_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98d5f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([source_pc_down])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bad191",
   "metadata": {},
   "source": [
    "#### Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4921af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the desired number of points for each batch \n",
    "# batch_size = len(batch_sizes) = 8\n",
    "batch_size = 54\n",
    "batch_sizes = [320]*batch_size  # Adjust as needed\n",
    "#batch_sizes[0] = 2048\n",
    "print(sum(batch_sizes))\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087bb5a",
   "metadata": {},
   "source": [
    "#### The function below creates for each point cloud batch non overlapping batch with fpfh parameters batch. When a batch is smaller then a batch_size the function adds the padding. At the end it makes the cuda points and fpfh tensors of floats for the PyTorch Transformer input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5029112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non overlaping batches\n",
    "\n",
    "def create_batches_with_padding(pcd, batch_sizes):\n",
    "    num_batches = len(batch_sizes)\n",
    "    batches_points = []\n",
    "    batches_fpfh = []\n",
    "    batch_start = 0\n",
    "    points = np.asarray(pcd.points)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_size = batch_sizes[i]\n",
    "        print('batch_size', batch_size)\n",
    "\n",
    "        # Initialize empty arrays for the current batch\n",
    "        batch_points = []\n",
    "        batch_fpfh = []\n",
    "        \n",
    "        # Cut the point cloud points to the size of the batch\n",
    "        if (len(points)-batch_start)>0:\n",
    "            batch_points = points[batch_start:(batch_start+batch_size)]\n",
    "        \n",
    "        # Calculate padding sizes\n",
    "        pad_points = batch_size - len(batch_points)\n",
    "        print('pad_points ', pad_points)\n",
    "\n",
    "        # Pad point cloud and FPFH to match the batch size\n",
    "        if len(batch_points)>0:\n",
    "            batch_points = np.pad(batch_points, [(0, pad_points), (0, 0)], mode='constant')\n",
    "\n",
    "\n",
    "            # FPFH for the points cut\n",
    "            batch_point_cloud = o3d.geometry.PointCloud()\n",
    "            batch_point_cloud.points = o3d.utility.Vector3dVector(batch_points)\n",
    "\n",
    "            batch_point_cloud.estimate_normals(\n",
    "                o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 2.0, max_nn=30))\n",
    "            fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "                batch_point_cloud, o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 5.0, max_nn=100))\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the batch to PyTorch tensors\n",
    "        batch_points = torch.FloatTensor(batch_points).cuda()#, dtype=torch.float32)\n",
    "        #batch_fpfh = torch.tensor(fpfh, dtype=torch.float32)\n",
    "        batch_fpfh = torch.FloatTensor(np.asarray(fpfh.data).copy()).T.cuda()\n",
    "\n",
    "        batches_points.append(batch_points)\n",
    "        batches_fpfh.append(batch_fpfh)\n",
    "        batch_start += batch_size\n",
    "\n",
    "    return batches_points, batches_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec9c5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  74\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "sour_batches_points, sour_batches_fpfh = create_batches_with_padding(source_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d9bcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(sour_batches_points))\n",
    "print(len(sour_batches_fpfh))\n",
    "print(sour_batches_points[1].shape)\n",
    "print(sour_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3f0ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  80\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "targ_batches_points, targ_batches_fpfh = create_batches_with_padding(target_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "992fcac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(targ_batches_points))\n",
    "print(len(targ_batches_fpfh))\n",
    "print(targ_batches_points[1].shape)\n",
    "print(targ_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe7f4a",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e55b79ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targ_batches_points[52][300].cpu().detach().numpy() != [0,0,0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55b08c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_batches_points[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d01c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_points = []\n",
    "target_test_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "for i in range(len(targ_batches_points)):\n",
    "    # Convert the aligned data to a NumPy array of shape (N, 3)\n",
    "    target_batch = targ_batches_points[i].cpu().detach().numpy()  # Assuming 'aligned_source' is a PyTorch tensor\n",
    "    target_batch_points = []\n",
    "    if len(target_batch)>0:\n",
    "        for point in target_batch:\n",
    "            if (point != [0,0,0]).all():\n",
    "                target_batch_points.append(point)\n",
    "    \n",
    "    if len(target_batch_points)>0:\n",
    "        # Store aligned data\n",
    "        target_test_points.append(target_batch_points)\n",
    "        # Create an Open3D point cloud and assign the aligned data\n",
    "        target_batch_pcd = o3d.geometry.PointCloud()\n",
    "        target_batch_pcd.points = o3d.utility.Vector3dVector(target_batch_points)\n",
    "        target_test_pcd += target_batch_pcd\n",
    "        o3d.visualization.draw_geometries([target_batch_pcd])\n",
    "    \n",
    "o3d.visualization.draw_geometries([target_test_pcd])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43a230",
   "metadata": {},
   "source": [
    "#### Simple PyTorch transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0059b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 0 with 320 points\n",
      "Target and source batch 0 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.9369685   1.3400666  -0.40309793]\n",
      "Targ centroid: [3.7136744e+05 7.9690088e+05 7.3881683e+01]\n",
      "Transl vector: [3.71367250e+05 7.96899223e+05 7.41528393e+01]\n",
      "Rotational MAE error xyz: [0.51383794 0.23968485 0.50780588], \n",
      "Translational MAE error xyz: [0.01499998 0.01499997 0.01498667]\n",
      "Rotational MAE: 0.4204428872414474, \n",
      "Translational MAE: 0.014995538735124428\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 1 with 320 points\n",
      "Target and source batch 1 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.3848673   0.64962715  0.73524034]\n",
      "Targ centroid: [3.7138044e+05 7.9690300e+05 7.3456146e+01]\n",
      "Transl vector: [3.71381883e+05 7.96903884e+05 7.33679388e+01]\n",
      "Rotational MAE error xyz: [0.34411054 0.57547547 0.45376091], \n",
      "Translational MAE error xyz: [0.015      0.015      0.01491629]\n",
      "Rotational MAE: 0.4577823056251975, \n",
      "Translational MAE: 0.014972099138402842\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 2 with 320 points\n",
      "Target and source batch 2 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.18046     0.06620617  1.1142538 ]\n",
      "Targ centroid: [3.7138675e+05 7.9690519e+05 7.4501556e+01]\n",
      "Transl vector: [3.71387186e+05 7.96906731e+05 7.42439795e+01]\n",
      "Rotational MAE error xyz: [0.50071965 0.69604972 0.42857376], \n",
      "Translational MAE error xyz: [0.01499998 0.01500001 0.01486269]\n",
      "Rotational MAE: 0.5417810450409469, \n",
      "Translational MAE: 0.014954229231069644\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 3 with 320 points\n",
      "Target and source batch 3 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.09259965  0.94414747 -0.8515483 ]\n",
      "Targ centroid: [3.7139084e+05 7.9690975e+05 7.3159683e+01]\n",
      "Transl vector: [3.71390118e+05 7.96908712e+05 7.33059995e+01]\n",
      "Rotational MAE error xyz: [0.42963686 0.24562076 0.50406678], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01492819]\n",
      "Rotational MAE: 0.39310813444321413, \n",
      "Translational MAE: 0.014976052491403736\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 4 with 320 points\n",
      "Target and source batch 4 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.9112617  -0.86828053 -0.04298102]\n",
      "Targ centroid: [3.713942e+05 7.969185e+05 7.191817e+01]\n",
      "Transl vector: [3.71392936e+05 7.96918633e+05 7.18793599e+01]\n",
      "Rotational MAE error xyz: [0.34120599 0.4992851  0.43817465], \n",
      "Translational MAE error xyz: [0.01499996 0.01499999 0.01499147]\n",
      "Rotational MAE: 0.42622191303227397, \n",
      "Translational MAE: 0.014997140023749987\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 5 with 320 points\n",
      "Target and source batch 5 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.07320551 -1.0843176   1.157523  ]\n",
      "Targ centroid: [3.713843e+05 7.969202e+05 7.174237e+01]\n",
      "Transl vector: [3.71384184e+05 7.96921752e+05 7.15046889e+01]\n",
      "Rotational MAE error xyz: [0.33368527 0.37837452 0.44800707], \n",
      "Translational MAE error xyz: [0.015      0.015      0.01485507]\n",
      "Rotational MAE: 0.3866889559395923, \n",
      "Translational MAE: 0.014951691370013785\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 6 with 320 points\n",
      "Target and source batch 6 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.3380084  -0.79561365 -0.5423944 ]\n",
      "Targ centroid: [3.7138375e+05 7.9691688e+05 7.2222092e+01]\n",
      "Transl vector: [3.71383478e+05 7.96915269e+05 7.24773593e+01]\n",
      "Rotational MAE error xyz: [0.78935791 0.8396656  0.47444488], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01497035]\n",
      "Rotational MAE: 0.7011561276181683, \n",
      "Translational MAE: 0.01499009702786109\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 7 with 320 points\n",
      "Target and source batch 7 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.194774  -0.2440186 -0.9507562]\n",
      "Targ centroid: [3.7138244e+05 7.9692712e+05 7.0783791e+01]\n",
      "Transl vector: [3.71382648e+05 7.96928639e+05 7.05474149e+01]\n",
      "Rotational MAE error xyz: [0.87605894 0.753379   0.59239948], \n",
      "Translational MAE error xyz: [0.01499998 0.01500001 0.01487485]\n",
      "Rotational MAE: 0.7406124726157707, \n",
      "Translational MAE: 0.014958279675963472\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 8 with 320 points\n",
      "Target and source batch 8 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.00394402 -0.7679306   0.7639867 ]\n",
      "Targ centroid: [3.7138672e+05 7.9692344e+05 7.1372337e+01]\n",
      "Transl vector: [3.71385928e+05 7.96924170e+05 7.12628185e+01]\n",
      "Rotational MAE error xyz: [0.48964605 0.69699438 0.44748042], \n",
      "Translational MAE error xyz: [0.01499998 0.015      0.01490863]\n",
      "Rotational MAE: 0.5447069495772385, \n",
      "Translational MAE: 0.014969537550191127\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 9 with 320 points\n",
      "Target and source batch 9 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.1246085  -0.9749627  -0.14964561]\n",
      "Targ centroid: [3.713839e+05 7.969283e+05 7.054253e+01]\n",
      "Transl vector: [3.71384340e+05 7.96929726e+05 7.03148331e+01]\n",
      "Rotational MAE error xyz: [0.77283774 0.52047442 0.55776708], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.01495986]\n",
      "Rotational MAE: 0.617026413779021, \n",
      "Translational MAE: 0.014986616595309551\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 10 with 320 points\n",
      "Target and source batch 10 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.7982431  -0.27715954  1.0754024 ]\n",
      "Targ centroid: [3.7138403e+05 7.9693031e+05 7.0203194e+01]\n",
      "Transl vector: [3.71384515e+05 7.96931578e+05 7.00152154e+01]\n",
      "Rotational MAE error xyz: [0.33260398 0.53183083 0.44422918], \n",
      "Translational MAE error xyz: [0.01499999 0.01500001 0.01486588]\n",
      "Rotational MAE: 0.4362213301308108, \n",
      "Translational MAE: 0.01495529343781449\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 11 with 320 points\n",
      "Target and source batch 11 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.1229354   0.14834167 -1.2712774 ]\n",
      "Targ centroid: [3.713887e+05 7.969316e+05 7.008757e+01]\n",
      "Transl vector: [3.71388438e+05 7.96929960e+05 7.03394116e+01]\n",
      "Rotational MAE error xyz: [0.4784853  0.6880777  0.44491351], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01489208]\n",
      "Rotational MAE: 0.5371588365067885, \n",
      "Translational MAE: 0.014964013023561878\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 12 with 320 points\n",
      "Target and source batch 12 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.0620382   1.2303782  -0.16833988]\n",
      "Targ centroid: [3.7138547e+05 7.9693088e+05 7.0210457e+01]\n",
      "Transl vector: [3.71384938e+05 7.96929347e+05 7.04446011e+01]\n",
      "Rotational MAE error xyz: [0.69529135 0.41330539 0.53998637], \n",
      "Translational MAE error xyz: [0.01499997 0.01499997 0.01500701]\n",
      "Rotational MAE: 0.5495277039734677, \n",
      "Translational MAE: 0.01500231688150972\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 13 with 320 points\n",
      "Target and source batch 13 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.291341   1.269853  -0.9785123]\n",
      "Targ centroid: [3.713842e+05 7.969276e+05 7.104871e+01]\n",
      "Transl vector: [3.71383206e+05 7.96926338e+05 7.12371266e+01]\n",
      "Rotational MAE error xyz: [0.5184053  0.2459599  0.51341012], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01491728]\n",
      "Rotational MAE: 0.4259251070294665, \n",
      "Translational MAE: 0.014972408548323874\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 14 with 320 points\n",
      "Target and source batch 14 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.4702584  -0.31053165 -0.15972684]\n",
      "Targ centroid: [3.713852e+05 7.969276e+05 7.120427e+01]\n",
      "Transl vector: [3.71385260e+05 7.96928199e+05 7.11122873e+01]\n",
      "Rotational MAE error xyz: [0.78950131 0.54290761 0.56615942], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.0149735 ]\n",
      "Rotational MAE: 0.6328561144321352, \n",
      "Translational MAE: 0.01499116471437791\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 15 with 320 points\n",
      "Target and source batch 15 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.15578377  0.85469705 -1.0104814 ]\n",
      "Targ centroid: [3.7138312e+05 7.9692762e+05 7.0983849e+01]\n",
      "Transl vector: [3.71382096e+05 7.96926790e+05 7.11228081e+01]\n",
      "Rotational MAE error xyz: [0.44969632 0.23866862 0.49600893], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01490866]\n",
      "Rotational MAE: 0.394791289134476, \n",
      "Translational MAE: 0.014969538755112671\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 16 with 320 points\n",
      "Target and source batch 16 with 320 points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating errors...\n",
      "Sour centroid: [-1.2606348   0.39213404  0.86850023]\n",
      "Targ centroid: [3.713884e+05 7.969263e+05 7.143745e+01]\n",
      "Transl vector: [3.71388298e+05 7.96924754e+05 7.16745844e+01]\n",
      "Rotational MAE error xyz: [0.86452867 0.69193159 0.58800932], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01493422]\n",
      "Rotational MAE: 0.7148231943268838, \n",
      "Translational MAE: 0.014978059539020037\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 17 with 320 points\n",
      "Target and source batch 17 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.36253163  0.51300716 -0.875539  ]\n",
      "Targ centroid: [3.7137691e+05 7.9692844e+05 7.0720100e+01]\n",
      "Transl vector: [3.71376783e+05 7.96927380e+05 7.08862834e+01]\n",
      "Rotational MAE error xyz: [0.33886444 0.46812981 0.44133815], \n",
      "Translational MAE error xyz: [0.01499999 0.01499999 0.01492532]\n",
      "Rotational MAE: 0.41611079840260734, \n",
      "Translational MAE: 0.014975098823191908\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 18 with 320 points\n",
      "Target and source batch 18 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.37106156 -0.30420518 -0.06685612]\n",
      "Targ centroid: [3.713643e+05 7.969290e+05 6.983460e+01]\n",
      "Transl vector: [3.71363844e+05 7.96928877e+05 6.98451018e+01]\n",
      "Rotational MAE error xyz: [0.38352696 0.61698024 0.44857943], \n",
      "Translational MAE error xyz: [0.01499998 0.015      0.01499395]\n",
      "Rotational MAE: 0.48302887859833293, \n",
      "Translational MAE: 0.014997976556461005\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 19 with 320 points\n",
      "Target and source batch 19 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.37505585  0.90400153 -1.2790569 ]\n",
      "Targ centroid: [3.7136900e+05 7.9693275e+05 6.9786575e+01]\n",
      "Transl vector: [3.71369185e+05 7.96931167e+05 7.00209284e+01]\n",
      "Rotational MAE error xyz: [0.33920088 0.51808912 0.44583845], \n",
      "Translational MAE error xyz: [0.015      0.01499998 0.01488893]\n",
      "Rotational MAE: 0.43437614651217565, \n",
      "Translational MAE: 0.014962967386154257\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 20 with 320 points\n",
      "Target and source batch 20 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.9615847  -0.30958885  1.2711736 ]\n",
      "Targ centroid: [3.7136816e+05 7.9692888e+05 7.0703758e+01]\n",
      "Transl vector: [3.71369400e+05 7.96929909e+05 7.05663489e+01]\n",
      "Rotational MAE error xyz: [0.34305495 0.27252152 0.45976413], \n",
      "Translational MAE error xyz: [0.01500001 0.01500001 0.01485177]\n",
      "Rotational MAE: 0.3584468674201291, \n",
      "Translational MAE: 0.014950594537351473\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 21 with 320 points\n",
      "Target and source batch 21 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.95585144  0.56691396  0.38893765]\n",
      "Targ centroid: [3.7136475e+05 7.9693525e+05 6.9765808e+01]\n",
      "Transl vector: [3.71364438e+05 7.96934126e+05 6.99272445e+01]\n",
      "Rotational MAE error xyz: [0.84426397 0.64544245 0.57797196], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01497564]\n",
      "Rotational MAE: 0.6892261260226845, \n",
      "Translational MAE: 0.014991865755541406\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 22 with 320 points\n",
      "Target and source batch 22 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.3417797 -0.755472   1.097252 ]\n",
      "Targ centroid: [3.7136356e+05 7.9693862e+05 6.9336136e+01]\n",
      "Transl vector: [3.71363169e+05 7.96939932e+05 6.91639530e+01]\n",
      "Rotational MAE error xyz: [0.376483   0.61388185 0.45459094], \n",
      "Translational MAE error xyz: [0.01499999 0.01500001 0.0148636 ]\n",
      "Rotational MAE: 0.4816519295881772, \n",
      "Translational MAE: 0.01495452906021006\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 23 with 320 points\n",
      "Target and source batch 23 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.1446481  -1.2375876   0.09293967]\n",
      "Targ centroid: [3.7136231e+05 7.9693756e+05 6.9775833e+01]\n",
      "Transl vector: [3.71363072e+05 7.96939060e+05 6.95988252e+01]\n",
      "Rotational MAE error xyz: [0.77062051 0.51783414 0.55960078], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.01497097]\n",
      "Rotational MAE: 0.6160184782077014, \n",
      "Translational MAE: 0.014990320257294994\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 24 with 320 points\n",
      "Target and source batch 24 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.7895671  -1.3667768   0.57721055]\n",
      "Targ centroid: [3.7136697e+05 7.9693888e+05 6.9603592e+01]\n",
      "Transl vector: [3.71367664e+05 7.96940392e+05 6.94047000e+01]\n",
      "Rotational MAE error xyz: [0.60373412 0.30285896 0.52840732], \n",
      "Translational MAE error xyz: [0.015     0.015     0.0149166]\n",
      "Rotational MAE: 0.47833346264439197, \n",
      "Translational MAE: 0.014972199653855968\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 25 with 320 points\n",
      "Target and source batch 25 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.1639068   0.07110546 -1.2350127 ]\n",
      "Targ centroid: [3.7136912e+05 7.9693825e+05 6.9600128e+01]\n",
      "Transl vector: [3.71368504e+05 7.96936685e+05 6.98207648e+01]\n",
      "Rotational MAE error xyz: [0.38786259 0.62002485 0.44627248], \n",
      "Translational MAE error xyz: [0.01499996 0.01499998 0.01489182]\n",
      "Rotational MAE: 0.48471997104151665, \n",
      "Translational MAE: 0.014963924036076631\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 26 with 320 points\n",
      "Target and source batch 26 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.392451   -0.90484315  0.51239204]\n",
      "Targ centroid: [3.7136384e+05 7.9693588e+05 6.9966347e+01]\n",
      "Transl vector: [3.71362771e+05 7.96936161e+05 6.99113987e+01]\n",
      "Rotational MAE error xyz: [0.51504951 0.71519653 0.44962566], \n",
      "Translational MAE error xyz: [0.01499997 0.01499999 0.01493938]\n",
      "Rotational MAE: 0.5599572316778564, \n",
      "Translational MAE: 0.014979782425894215\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 27 with 320 points\n",
      "Target and source batch 27 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.03166581  1.1498898  -1.1182241 ]\n",
      "Targ centroid: [3.713677e+05 7.969383e+05 6.958878e+01]\n",
      "Transl vector: [3.71366940e+05 7.96936904e+05 6.97629653e+01]\n",
      "Rotational MAE error xyz: [0.34232759 0.25780043 0.49480973], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01489919]\n",
      "Rotational MAE: 0.3649792508330541, \n",
      "Translational MAE: 0.014966383791020785\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 28 with 320 points\n",
      "Target and source batch 28 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 29 with 320 points\n",
      "Target and source batch 29 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 30 with 320 points\n",
      "Target and source batch 30 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 31 with 320 points\n",
      "Target and source batch 31 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 32 with 320 points\n",
      "Target and source batch 32 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 33 with 320 points\n",
      "Target and source batch 33 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 34 with 320 points\n",
      "Target and source batch 34 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 35 with 320 points\n",
      "Target and source batch 35 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 36 with 320 points\n",
      "Target and source batch 36 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 37 with 320 points\n",
      "Target and source batch 37 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 38 with 320 points\n",
      "Target and source batch 38 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 39 with 320 points\n",
      "Target and source batch 39 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 40 with 320 points\n",
      "Target and source batch 40 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 41 with 320 points\n",
      "Target and source batch 41 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 42 with 320 points\n",
      "Target and source batch 42 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 43 with 320 points\n",
      "Target and source batch 43 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 44 with 320 points\n",
      "Target and source batch 44 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 45 with 320 points\n",
      "Target and source batch 45 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 46 with 320 points\n",
      "Target and source batch 46 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 47 with 320 points\n",
      "Target and source batch 47 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 48 with 320 points\n",
      "Target and source batch 48 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 49 with 320 points\n",
      "Target and source batch 49 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 50 with 320 points\n",
      "Target and source batch 50 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 51 with 320 points\n",
      "Target and source batch 51 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 52 with 320 points\n",
      "Target and source batch 52 with 0 points\n",
      "input_dim = 0\n",
      "d_model = 3\n",
      "Processing target batch 53 with 0 points\n",
      "Target and source batch 53 with 0 points\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Transformer\n",
    "\n",
    "# model = Transformer().cuda()\n",
    "# src = torch.rand((10, 32, 512)).float().cuda()\n",
    "# tgt = torch.rand((20, 32, 512)).float().cuda()\n",
    "\n",
    "# print(model(src, tgt).shape)\n",
    "\n",
    "aligned_pcd_points = []\n",
    "results = []\n",
    "aligned_point_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "for i in range(len(targ_batches_points)):\n",
    "    # Initialize the transformer model\n",
    "    input_dim = len(targ_batches_points[i])*2 # Define your input dimension\n",
    "    print(f'input_dim = {input_dim}')\n",
    "    num_heads = 3   # Number of attention heads\n",
    "    num_layers = 6  # Number of transformer layers\n",
    "    d_model = 3   # Dimension of the embedding vectors\n",
    "    print(f'd_model = {d_model}')\n",
    "    hidden_dim = 256 # Hidden dimension in feed-forward layers\n",
    "    \n",
    "    target_points = targ_batches_points[i]\n",
    "    target_fpfh = targ_batches_fpfh[i]\n",
    "    print(f'Processing target batch {i} with {len(target_points)} points')\n",
    "\n",
    "    model = Transformer(d_model=d_model, nhead=num_heads).cuda()\n",
    "    source_points = sour_batches_points[i]\n",
    "    source_fpfh = sour_batches_fpfh[i]\n",
    "    print(f'Target and source batch {i} with {len(source_points)} points')\n",
    "\n",
    "    if len(target_points)>0 and len(source_points)>0:\n",
    "        result = model(source_points, target_points)\n",
    "        results.append(result)\n",
    "        # print(result)\n",
    "        # Convert the aligned data to a NumPy array of shape (N, 3)\n",
    "        aligned_data_numpy = result.cpu().detach().numpy()  # Assuming 'aligned_source' is a PyTorch tensor\n",
    "\n",
    "        aligned_batch_points = []\n",
    "        if len(aligned_data_numpy)>0:\n",
    "            for point in aligned_data_numpy:\n",
    "                if (point != [0,0,0]).all():\n",
    "                    aligned_batch_points.append(point)\n",
    "\n",
    "        if len(aligned_batch_points)>0:\n",
    "            # Store aligned data\n",
    "            aligned_pcd_points.append(aligned_batch_points)\n",
    "            # Create an Open3D point cloud and assign the aligned data\n",
    "            aligned_batch_pcd = o3d.geometry.PointCloud()\n",
    "            aligned_batch_pcd.points = o3d.utility.Vector3dVector(aligned_batch_points)\n",
    "            aligned_point_cloud += aligned_batch_pcd\n",
    "            o3d.visualization.draw_geometries([aligned_batch_pcd])\n",
    "            \n",
    "        rot_err, transl_err = registration_error(aligned_data_numpy, targ_batches_points[i].cpu().detach().numpy())\n",
    "        print(f'Rotational MAE error xyz: {rot_err}, \\nTranslational MAE error xyz: {transl_err}')\n",
    "        print(f'Rotational MAE: {np.mean(rot_err)}, \\nTranslational MAE: {np.mean(transl_err)}')\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "o3d.visualization.draw_geometries([aligned_point_cloud])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6eed45",
   "metadata": {},
   "source": [
    "### Transformer 2 (with overlapping batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970354bd",
   "metadata": {},
   "source": [
    "#### Overlaping batches with 20 points overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8abd792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8886, 3)\n",
      "(16880, 3)\n"
     ]
    }
   ],
   "source": [
    "print(source_pc_down_points.shape)\n",
    "print(target_pc_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf32326d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the desired number of points for each batch \n",
    "# batch_size = len(batch_sizes) = 8\n",
    "batch_size = 58\n",
    "batch_sizes = [320]*batch_size  # Adjust as needed\n",
    "#batch_sizes[0] = 2048\n",
    "print(sum(batch_sizes)-20*batch_size)\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64b34add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaping batches\n",
    "\n",
    "def create_overlapping_batches_with_padding(pcd, batch_sizes):\n",
    "    num_batches = len(batch_sizes)\n",
    "    batches_points = []\n",
    "    batches_fpfh = []\n",
    "    batch_start = 0\n",
    "    points = np.asarray(pcd.points)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_size = batch_sizes[i]\n",
    "        print('batch_size', batch_size)\n",
    "\n",
    "        # Initialize empty arrays for the current batch\n",
    "        batch_points = []\n",
    "        batch_fpfh = []\n",
    "        \n",
    "        # Cut the point cloud points to the size of the batch\n",
    "        if (len(points)-batch_start)>0:\n",
    "            batch_points = points[batch_start:(batch_start+batch_size)]\n",
    "        \n",
    "        # Calculate padding sizes\n",
    "        pad_points = batch_size - len(batch_points)\n",
    "        print('pad_points ', pad_points)\n",
    "\n",
    "        # Pad point cloud and FPFH to match the batch size\n",
    "        if len(batch_points)>0:\n",
    "            batch_points = np.pad(batch_points, [(0, pad_points), (0, 0)], mode='constant')\n",
    "\n",
    "\n",
    "            # FPFH for the points cut\n",
    "            batch_point_cloud = o3d.geometry.PointCloud()\n",
    "            batch_point_cloud.points = o3d.utility.Vector3dVector(batch_points)\n",
    "\n",
    "            batch_point_cloud.estimate_normals(\n",
    "                o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 2.0, max_nn=30))\n",
    "            fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "                batch_point_cloud, o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 5.0, max_nn=100))\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the batch to PyTorch tensors\n",
    "        batch_points = torch.FloatTensor(batch_points).cuda()#, dtype=torch.float32)\n",
    "        #batch_fpfh = torch.tensor(fpfh, dtype=torch.float32)\n",
    "        batch_fpfh = torch.FloatTensor(np.asarray(fpfh.data).copy()).T.cuda()\n",
    "\n",
    "        batches_points.append(batch_points)\n",
    "        batches_fpfh.append(batch_fpfh)\n",
    "        batch_start += (batch_size - 20)\n",
    "\n",
    "    return batches_points, batches_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b1f4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  134\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "sour_batches_points, sour_batches_fpfh = create_overlapping_batches_with_padding(source_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8be0c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "58\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(sour_batches_points))\n",
    "print(len(sour_batches_fpfh))\n",
    "print(sour_batches_points[1].shape)\n",
    "print(sour_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48e7e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  240\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "targ_batches_points, targ_batches_fpfh = create_overlapping_batches_with_padding(target_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "075e59ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "58\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(targ_batches_points))\n",
    "print(len(targ_batches_fpfh))\n",
    "print(targ_batches_points[1].shape)\n",
    "print(targ_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704533",
   "metadata": {},
   "source": [
    "#### Transformer result loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f8d9845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 0 with 320 points\n",
      "Target and source batch 0 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.13274597 -1.0261794   0.8934337 ]\n",
      "Targ centroid: [3.7136744e+05 7.9690088e+05 7.3881683e+01]\n",
      "Transl vector: [3.71368604e+05 7.96901578e+05 7.37681830e+01]\n",
      "Rotational MAE error xyz: [0.64927631 0.35285773 0.53253294], \n",
      "Translational MAE error xyz: [0.01500002 0.015      0.01489832]\n",
      "Rotational MAE: 0.5115556604206045, \n",
      "Translational MAE: 0.014966112497844757\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 1 with 320 points\n",
      "Target and source batch 1 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.2145312  0.8861224  0.3284089]\n",
      "Targ centroid: [3.7137991e+05 7.9690256e+05 7.3312202e+01]\n",
      "Transl vector: [3.71378978e+05 7.96901343e+05 7.34488540e+01]\n",
      "Rotational MAE error xyz: [0.87561226 0.73625275 0.58231752], \n",
      "Translational MAE error xyz: [0.01499996 0.01499998 0.01498044]\n",
      "Rotational MAE: 0.731394176559853, \n",
      "Translational MAE: 0.014993460636497652\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 2 with 320 points\n",
      "Target and source batch 2 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.9260014   0.39307898  0.5329226 ]\n",
      "Targ centroid: [3.7138628e+05 7.9690338e+05 7.4785614e+01]\n",
      "Transl vector: [3.71387404e+05 7.96903199e+05 7.48448202e+01]\n",
      "Rotational MAE error xyz: [0.34541228 0.21974086 0.45307172], \n",
      "Translational MAE error xyz: [0.015      0.01499999 0.01495268]\n",
      "Rotational MAE: 0.3394082899586743, \n",
      "Translational MAE: 0.01498422621412969\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 3 with 320 points\n",
      "Target and source batch 3 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.59079087 -1.2966591   0.70586807]\n",
      "Targ centroid: [3.7139116e+05 7.9691050e+05 7.3291466e+01]\n",
      "Transl vector: [3.71391420e+05 7.96912052e+05 7.30684247e+01]\n",
      "Rotational MAE error xyz: [0.40073499 0.24886885 0.50424201], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.01490526]\n",
      "Rotational MAE: 0.3846152859191076, \n",
      "Translational MAE: 0.014968416989200572\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 4 with 320 points\n",
      "Target and source batch 4 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.268745   -0.16918072 -1.0995648 ]\n",
      "Targ centroid: [3.7139428e+05 7.9691662e+05 7.2445557e+01]\n",
      "Transl vector: [3.71394258e+05 7.96914958e+05 7.27047046e+01]\n",
      "Rotational MAE error xyz: [0.65744217 0.79648841 0.45245936], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.0149138 ]\n",
      "Rotational MAE: 0.6354633127865236, \n",
      "Translational MAE: 0.014971253209113753\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 5 with 320 points\n",
      "Target and source batch 5 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.26760322  1.0198038  -1.2874069 ]\n",
      "Targ centroid: [3.713874e+05 7.969191e+05 7.157712e+01]\n",
      "Transl vector: [3.71387208e+05 7.96917492e+05 7.18265294e+01]\n",
      "Rotational MAE error xyz: [0.33637725 0.34712635 0.45028549], \n",
      "Translational MAE error xyz: [0.01499999 0.01499998 0.01489239]\n",
      "Rotational MAE: 0.3779296987491367, \n",
      "Translational MAE: 0.014964119143641473\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 6 with 320 points\n",
      "Target and source batch 6 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.2707042  -1.0565546  -0.21414971]\n",
      "Targ centroid: [3.7138588e+05 7.9691756e+05 7.2379349e+01]\n",
      "Transl vector: [3.71386102e+05 7.96919192e+05 7.21171968e+01]\n",
      "Rotational MAE error xyz: [0.73203789 0.45825666 0.55196098], \n",
      "Translational MAE error xyz: [0.01499998 0.01500001 0.01495063]\n",
      "Rotational MAE: 0.5807518462051585, \n",
      "Translational MAE: 0.014983537663633265\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 7 with 320 points\n",
      "Target and source batch 7 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.7764642  -0.8103296   0.03386514]\n",
      "Targ centroid: [3.7138128e+05 7.9692075e+05 7.1480789e+01]\n",
      "Transl vector: [3.71381782e+05 7.96921742e+05 7.13225873e+01]\n",
      "Rotational MAE error xyz: [0.77486999 0.52599208 0.55573142], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.01497983]\n",
      "Rotational MAE: 0.6188644979058701, \n",
      "Translational MAE: 0.014993275334538354\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 8 with 320 points\n",
      "Target and source batch 8 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.1884663  -0.11611374 -1.0723522 ]\n",
      "Targ centroid: [3.7138844e+05 7.9692631e+05 7.1204544e+01]\n",
      "Transl vector: [3.71388565e+05 7.96924731e+05 7.14438986e+01]\n",
      "Rotational MAE error xyz: [0.68173038 0.80942845 0.45919914], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01491306]\n",
      "Rotational MAE: 0.6501193236467354, \n",
      "Translational MAE: 0.0149710078829624\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 9 with 320 points\n",
      "Target and source batch 9 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.2604096  1.3079956 -1.0475862]\n",
      "Targ centroid: [3.7138306e+05 7.9692600e+05 7.0716957e+01]\n",
      "Transl vector: [3.71383010e+05 7.96924324e+05 7.09728579e+01]\n",
      "Rotational MAE error xyz: [0.33191959 0.24621668 0.46225253], \n",
      "Translational MAE error xyz: [0.01499999 0.01499997 0.0149168 ]\n",
      "Rotational MAE: 0.34679626870764424, \n",
      "Translational MAE: 0.014972256412916581\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 10 with 320 points\n",
      "Target and source batch 10 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.39966935  0.2077597  -0.6074291 ]\n",
      "Targ centroid: [3.7138488e+05 7.9692731e+05 7.0580925e+01]\n",
      "Transl vector: [3.71384299e+05 7.96926829e+05 7.06566190e+01]\n",
      "Rotational MAE error xyz: [0.33257972 0.24499247 0.4586117 ], \n",
      "Translational MAE error xyz: [0.01499998 0.01499999 0.01494377]\n",
      "Rotational MAE: 0.3453946307571305, \n",
      "Translational MAE: 0.014981247735576715\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 11 with 320 points\n",
      "Target and source batch 11 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.5804862  0.9403291 -0.3598428]\n",
      "Targ centroid: [3.7138556e+05 7.9693362e+05 6.9873123e+01]\n",
      "Transl vector: [3.71384491e+05 7.96933181e+05 6.99386026e+01]\n",
      "Rotational MAE error xyz: [0.85898558 0.68618692 0.57931788], \n",
      "Translational MAE error xyz: [0.01499997 0.01499999 0.0149685 ]\n",
      "Rotational MAE: 0.708163458292668, \n",
      "Translational MAE: 0.01498948442853084\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 12 with 320 points\n",
      "Target and source batch 12 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.2574753   0.88339376  0.3740812 ]\n",
      "Targ centroid: [3.7138581e+05 7.9693088e+05 7.0177345e+01]\n",
      "Transl vector: [3.71386519e+05 7.96929476e+05 7.03923811e+01]\n",
      "Rotational MAE error xyz: [0.52172645 0.244605   0.50755907], \n",
      "Translational MAE error xyz: [0.01499999 0.01499998 0.01498307]\n",
      "Rotational MAE: 0.42463017275940246, \n",
      "Translational MAE: 0.014994346932967856\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 13 with 320 points\n",
      "Target and source batch 13 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.39986378  1.2638738  -0.86401016]\n",
      "Targ centroid: [3.7138694e+05 7.9692969e+05 7.0531021e+01]\n",
      "Transl vector: [3.71385949e+05 7.96928467e+05 7.07249503e+01]\n",
      "Rotational MAE error xyz: [0.57792388 0.27183107 0.51885706], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01492928]\n",
      "Rotational MAE: 0.45620400416255896, \n",
      "Translational MAE: 0.014976408258961835\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 14 with 320 points\n",
      "Target and source batch 14 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.30937976  0.9435178  -1.2528979 ]\n",
      "Targ centroid: [3.7138538e+05 7.9692806e+05 7.1032120e+01]\n",
      "Transl vector: [3.71384797e+05 7.96926587e+05 7.12424361e+01]\n",
      "Rotational MAE error xyz: [0.33953603 0.2494962  0.464523  ], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01489104]\n",
      "Rotational MAE: 0.35118507547374095, \n",
      "Translational MAE: 0.014963666458076936\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 15 with 320 points\n",
      "Target and source batch 15 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.32898456  1.3199632  -0.99097884]\n",
      "Targ centroid: [3.7138466e+05 7.9692744e+05 7.1202988e+01]\n",
      "Transl vector: [3.71383861e+05 7.96925973e+05 7.14373116e+01]\n",
      "Rotational MAE error xyz: [0.44858657 0.23956284 0.50026589], \n",
      "Translational MAE error xyz: [0.01499998 0.01499997 0.01492098]\n",
      "Rotational MAE: 0.3961384327980402, \n",
      "Translational MAE: 0.014973643321946227\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 16 with 320 points\n",
      "Target and source batch 16 with 320 points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating errors...\n",
      "Sour centroid: [-0.07806135  1.1417259  -1.0636644 ]\n",
      "Targ centroid: [3.7138312e+05 7.9692762e+05 7.0983849e+01]\n",
      "Transl vector: [3.71382201e+05 7.96926382e+05 7.11906131e+01]\n",
      "Rotational MAE error xyz: [0.42560011 0.23866861 0.49220298], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01491026]\n",
      "Rotational MAE: 0.3854905656437612, \n",
      "Translational MAE: 0.014970074106599299\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 17 with 320 points\n",
      "Target and source batch 17 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.98806936 -0.06574559  1.0538146 ]\n",
      "Targ centroid: [3.7138906e+05 7.9692588e+05 7.1603203e+01]\n",
      "Transl vector: [3.71389712e+05 7.96924599e+05 7.18078923e+01]\n",
      "Rotational MAE error xyz: [0.83492381 0.62316414 0.57832595], \n",
      "Translational MAE error xyz: [0.01499999 0.01499999 0.01491183]\n",
      "Rotational MAE: 0.6788046321000124, \n",
      "Translational MAE: 0.014970604708726956\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 18 with 320 points\n",
      "Target and source batch 18 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.3414184  -0.9272639  -0.41415483]\n",
      "Targ centroid: [3.7137806e+05 7.9692800e+05 7.0832832e+01]\n",
      "Transl vector: [3.71378783e+05 7.96929501e+05 7.05912762e+01]\n",
      "Rotational MAE error xyz: [0.85443562 0.66783308 0.58458762], \n",
      "Translational MAE error xyz: [0.01499999 0.01500001 0.01493066]\n",
      "Rotational MAE: 0.7022854412099542, \n",
      "Translational MAE: 0.014976882856805108\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 19 with 320 points\n",
      "Target and source batch 19 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.95012903  0.38863552 -1.3387641 ]\n",
      "Targ centroid: [3.7136406e+05 7.9692894e+05 6.9901596e+01]\n",
      "Transl vector: [3.71363625e+05 7.96927325e+05 7.01335736e+01]\n",
      "Rotational MAE error xyz: [0.34320381 0.56375878 0.44394335], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01488257]\n",
      "Rotational MAE: 0.4503019790063784, \n",
      "Translational MAE: 0.014960840697773078\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 20 with 320 points\n",
      "Target and source batch 20 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.22240734  0.9455799  -1.167987  ]\n",
      "Targ centroid: [3.7136616e+05 7.9693238e+05 6.9586266e+01]\n",
      "Transl vector: [3.71365137e+05 7.96931258e+05 6.97321981e+01]\n",
      "Rotational MAE error xyz: [0.35285599 0.25289222 0.4932539 ], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01489087]\n",
      "Rotational MAE: 0.36633403607561377, \n",
      "Translational MAE: 0.014963609771188949\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 21 with 320 points\n",
      "Target and source batch 21 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-1.2503005   1.1208005   0.12949972]\n",
      "Targ centroid: [3.7136759e+05 7.9693075e+05 7.0098618e+01]\n",
      "Transl vector: [3.71367504e+05 7.96929084e+05 7.03315678e+01]\n",
      "Rotational MAE error xyz: [0.67496739 0.38447154 0.54388665], \n",
      "Translational MAE error xyz: [0.01499997 0.01499997 0.01501104]\n",
      "Rotational MAE: 0.5344418581675755, \n",
      "Translational MAE: 0.01500366212103481\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 22 with 320 points\n",
      "Target and source batch 22 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.82829046  0.5193707  -1.3476615 ]\n",
      "Targ centroid: [3.7136706e+05 7.9693156e+05 7.0321976e+01]\n",
      "Transl vector: [3.71366651e+05 7.96929969e+05 7.05710636e+01]\n",
      "Rotational MAE error xyz: [0.33939358 0.51592597 0.43916217], \n",
      "Translational MAE error xyz: [0.01499997 0.01499998 0.01488415]\n",
      "Rotational MAE: 0.431493905476922, \n",
      "Translational MAE: 0.014961368187427574\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 23 with 320 points\n",
      "Target and source batch 23 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.3222004  1.3346636 -1.0124633]\n",
      "Targ centroid: [3.7136616e+05 7.9693725e+05 6.9542038e+01]\n",
      "Transl vector: [3.71365496e+05 7.96935689e+05 6.97383884e+01]\n",
      "Rotational MAE error xyz: [0.39364372 0.25875925 0.50087858], \n",
      "Translational MAE error xyz: [0.01499998 0.01499997 0.01491274]\n",
      "Rotational MAE: 0.384427183030372, \n",
      "Translational MAE: 0.014970898289391653\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 24 with 320 points\n",
      "Target and source batch 24 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.22166789  1.0487592  -0.8270912 ]\n",
      "Targ centroid: [3.7136306e+05 7.9693762e+05 6.9832321e+01]\n",
      "Transl vector: [3.71362150e+05 7.96936632e+05 6.99571080e+01]\n",
      "Rotational MAE error xyz: [0.55699589 0.25360005 0.52390974], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01492508]\n",
      "Rotational MAE: 0.44483522518363805, \n",
      "Translational MAE: 0.014975013745289999\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 25 with 320 points\n",
      "Target and source batch 25 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.6214529   0.91938484 -0.2979315 ]\n",
      "Targ centroid: [3.713612e+05 7.969403e+05 6.933580e+01]\n",
      "Transl vector: [3.71360935e+05 7.96939198e+05 6.94568981e+01]\n",
      "Rotational MAE error xyz: [0.56062907 0.26805965 0.53039555], \n",
      "Translational MAE error xyz: [0.01499998 0.01499998 0.01498093]\n",
      "Rotational MAE: 0.45302809091237933, \n",
      "Translational MAE: 0.01499363100983162\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 26 with 320 points\n",
      "Target and source batch 26 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.282763   -1.1010749  -0.18168834]\n",
      "Targ centroid: [3.7136956e+05 7.9693581e+05 7.0110771e+01]\n",
      "Transl vector: [3.71370287e+05 7.96937334e+05 6.98875826e+01]\n",
      "Rotational MAE error xyz: [0.81783186 0.59626468 0.56817829], \n",
      "Translational MAE error xyz: [0.01499999 0.015      0.01495668]\n",
      "Rotational MAE: 0.6607582789726689, \n",
      "Translational MAE: 0.014985556251278053\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 27 with 320 points\n",
      "Target and source batch 27 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.7689332   0.03934551  0.72958755]\n",
      "Targ centroid: [3.7136747e+05 7.9693788e+05 6.9566483e+01]\n",
      "Transl vector: [3.71368529e+05 7.96937905e+05 6.95811367e+01]\n",
      "Rotational MAE error xyz: [0.34342256 0.2570556  0.491525  ], \n",
      "Translational MAE error xyz: [0.01500001 0.015      0.01492333]\n",
      "Rotational MAE: 0.3640010536975797, \n",
      "Translational MAE: 0.01497444616120844\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 28 with 320 points\n",
      "Target and source batch 28 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 1.2197895 -0.6181923 -0.6015972]\n",
      "Targ centroid: [3.7136222e+05 7.9693881e+05 6.9480309e+01]\n",
      "Transl vector: [3.7136228e+05 7.9694029e+05 6.9267155e+01]\n",
      "Rotational MAE error xyz: [0.81475711 0.59014946 0.56786036], \n",
      "Translational MAE error xyz: [0.01499998 0.01500001 0.0149123 ]\n",
      "Rotational MAE: 0.6575889780757634, \n",
      "Translational MAE: 0.014970760595417937\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 29 with 320 points\n",
      "Target and source batch 29 with 320 points\n",
      "Calculating errors...\n",
      "Sour centroid: [ 0.76133764  0.5201757  -1.2815133 ]\n",
      "Targ centroid: [3.7136894e+05 7.9693762e+05 6.9837807e+01]\n",
      "Transl vector: [3.71367904e+05 7.96936441e+05 6.99918170e+01]\n",
      "Rotational MAE error xyz: [0.34069874 0.26329965 0.46513461], \n",
      "Translational MAE error xyz: [0.01499996 0.01499998 0.01488015]\n",
      "Rotational MAE: 0.3563776673780749, \n",
      "Translational MAE: 0.014960031694630541\n",
      "\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 30 with 320 points\n",
      "Target and source batch 30 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 31 with 320 points\n",
      "Target and source batch 31 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 32 with 320 points\n",
      "Target and source batch 32 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 33 with 320 points\n",
      "Target and source batch 33 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 34 with 320 points\n",
      "Target and source batch 34 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 35 with 320 points\n",
      "Target and source batch 35 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 36 with 320 points\n",
      "Target and source batch 36 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 37 with 320 points\n",
      "Target and source batch 37 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 38 with 320 points\n",
      "Target and source batch 38 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 39 with 320 points\n",
      "Target and source batch 39 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 40 with 320 points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target and source batch 40 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 41 with 320 points\n",
      "Target and source batch 41 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 42 with 320 points\n",
      "Target and source batch 42 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 43 with 320 points\n",
      "Target and source batch 43 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 44 with 320 points\n",
      "Target and source batch 44 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 45 with 320 points\n",
      "Target and source batch 45 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 46 with 320 points\n",
      "Target and source batch 46 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 47 with 320 points\n",
      "Target and source batch 47 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 48 with 320 points\n",
      "Target and source batch 48 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 49 with 320 points\n",
      "Target and source batch 49 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 50 with 320 points\n",
      "Target and source batch 50 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 51 with 320 points\n",
      "Target and source batch 51 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 52 with 320 points\n",
      "Target and source batch 52 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 53 with 320 points\n",
      "Target and source batch 53 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 54 with 320 points\n",
      "Target and source batch 54 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 55 with 320 points\n",
      "Target and source batch 55 with 0 points\n",
      "input_dim = 640\n",
      "d_model = 3\n",
      "Processing target batch 56 with 320 points\n",
      "Target and source batch 56 with 0 points\n",
      "input_dim = 0\n",
      "d_model = 3\n",
      "Processing target batch 57 with 0 points\n",
      "Target and source batch 57 with 0 points\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Transformer\n",
    "\n",
    "# model = Transformer().cuda()\n",
    "# src = torch.rand((10, 32, 512)).float().cuda()\n",
    "# tgt = torch.rand((20, 32, 512)).float().cuda()\n",
    "\n",
    "# print(model(src, tgt).shape)\n",
    "\n",
    "aligned_pcd_points = []\n",
    "results = []\n",
    "aligned_point_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "for i in range(len(targ_batches_points)):\n",
    "    # Initialize the transformer model\n",
    "    input_dim = len(targ_batches_points[i])*2 # Define your input dimension\n",
    "    print(f'input_dim = {input_dim}')\n",
    "    num_heads = 3   # Number of attention heads\n",
    "    num_layers = 6  # Number of transformer layers\n",
    "    d_model = 3   # Dimension of the embedding vectors\n",
    "    print(f'd_model = {d_model}')\n",
    "    hidden_dim = 256 # Hidden dimension in feed-forward layers\n",
    "    \n",
    "    target_points = targ_batches_points[i]\n",
    "    target_fpfh = targ_batches_fpfh[i]\n",
    "    print(f'Processing target batch {i} with {len(target_points)} points')\n",
    "\n",
    "    model = Transformer(d_model=d_model, nhead=num_heads).cuda()\n",
    "    source_points = sour_batches_points[i]\n",
    "    source_fpfh = sour_batches_fpfh[i]\n",
    "    print(f'Target and source batch {i} with {len(source_points)} points')\n",
    "\n",
    "    if len(target_points)>0 and len(source_points)>0:\n",
    "        result = model(source_points, target_points)\n",
    "        results.append(result)\n",
    "        # print(result)\n",
    "        # Convert the aligned data to a NumPy array of shape (N, 3)\n",
    "        aligned_data_numpy = result.cpu().detach().numpy()  # Assuming 'aligned_source' is a PyTorch tensor\n",
    "\n",
    "        aligned_batch_points = []\n",
    "        if len(aligned_data_numpy)>0:\n",
    "            for point in aligned_data_numpy:\n",
    "                if (point != [0,0,0]).all():\n",
    "                    aligned_batch_points.append(point)\n",
    "\n",
    "        if len(aligned_batch_points)>0:\n",
    "            # Store aligned data\n",
    "            aligned_pcd_points.append(aligned_batch_points)\n",
    "            # Create an Open3D point cloud and assign the aligned data\n",
    "            aligned_batch_pcd = o3d.geometry.PointCloud()\n",
    "            aligned_batch_pcd.points = o3d.utility.Vector3dVector(aligned_batch_points)\n",
    "            aligned_point_cloud += aligned_batch_pcd\n",
    "            o3d.visualization.draw_geometries([aligned_batch_pcd])\n",
    "            \n",
    "        rot_err, transl_err = registration_error(aligned_data_numpy, targ_batches_points[i].cpu().detach().numpy())\n",
    "        print(f'Rotational MAE error xyz: {rot_err}, \\nTranslational MAE error xyz: {transl_err}')\n",
    "        print(f'Rotational MAE: {np.mean(rot_err)}, \\nTranslational MAE: {np.mean(transl_err)}')\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "o3d.visualization.draw_geometries([aligned_point_cloud])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ca15b",
   "metadata": {},
   "source": [
    "### Transformer 3 self made (with overlapping batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cfd0d",
   "metadata": {},
   "source": [
    "Tutorial: https://deeplearning.neuromatch.io/tutorials/W2D5_AttentionAndTransformers/student/W2D5_Tutorial1.html#training-the-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ebcf01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "  \"\"\" Scaled dot product attention. \"\"\"\n",
    "\n",
    "  def __init__(self, dropout, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Scaled Dot Product Attention Instance.\n",
    "\n",
    "    Args:\n",
    "      dropout: Integer\n",
    "        Specifies probability of dropout hyperparameter\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(DotProductAttention, self).__init__(**kwargs)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def calculate_score(self, queries, keys):\n",
    "      \"\"\"\n",
    "      Compute the score between queries and keys.\n",
    "\n",
    "      Args:\n",
    "      queries: Tensor\n",
    "        Query is your search tag/Question\n",
    "        Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n",
    "      keys: Tensor\n",
    "        Descriptions associated with the database for instance\n",
    "        Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n",
    "      \"\"\"\n",
    "      return torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(queries.shape[-1])\n",
    "\n",
    "  def forward(self, queries, keys, values, b, h, t, k):\n",
    "    \"\"\"\n",
    "    Compute dot products. This is the same operation for each head,\n",
    "    so we can fold the heads into the batch dimension and use torch.bmm\n",
    "    Note: .contiguous() doesn't change the actual shape of the data,\n",
    "    but it rearranges the tensor in memory, which will help speed up the computation\n",
    "    for this batch matrix multiplication.\n",
    "    .transpose() is used to change the shape of a tensor. It returns a new tensor\n",
    "    that shares the data with the original tensor. It can only swap two dimensions.\n",
    "\n",
    "    Args:\n",
    "      queries: Tensor\n",
    "        Query is your search tag/Question\n",
    "        Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n",
    "      keys: Tensor\n",
    "        Descriptions associated with the database for instance\n",
    "        Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n",
    "      values: Tensor\n",
    "        Values are returned results on the query\n",
    "        Shape of `values`: (`batch_size`, head, no. of key-value pairs,  `k`)\n",
    "      b: Integer\n",
    "        Batch size\n",
    "      h: Integer\n",
    "        Number of heads\n",
    "      t: Integer\n",
    "        Number of keys/queries/values (for simplicity, let's assume they have the same sizes)\n",
    "      k: Integer\n",
    "        Embedding size\n",
    "\n",
    "    Returns:\n",
    "      out: Tensor\n",
    "        Matrix Multiplication between the keys, queries and values.\n",
    "    \"\"\"\n",
    "    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "\n",
    "    # Matrix Multiplication between the keys and queries\n",
    "    score = self.calculate_score(queries, keys)  # size: (b * h, t, t)\n",
    "    softmax_weights = F.softmax(score, dim=2)  # row-wise normalization of weights\n",
    "\n",
    "    # Matrix Multiplication between the output of the key and queries multiplication and values.\n",
    "    out = torch.bmm(self.dropout(softmax_weights), values).view(b, h, t, k)  # rearrange h and t dims\n",
    "    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a24c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  \"\"\"  Multi-head self attention layer. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initiates the following attributes:\n",
    "    to_keys: Transforms input to k x k*heads key vectors\n",
    "    to_queries: Transforms input to k x k*heads query vectors\n",
    "    to_values: Transforms input to k x k*heads value vectors\n",
    "    unify_heads: combines queries, keys and values to a single vector\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Size of attention embeddings\n",
    "      heads: Integer\n",
    "        Number of attention heads\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.k, self.heads = k, heads\n",
    "\n",
    "    self.to_keys = nn.Linear(k, k * heads, bias=False)\n",
    "    self.to_queries = nn.Linear(k, k * heads, bias=False)\n",
    "    self.to_values = nn.Linear(k, k * heads, bias=False)\n",
    "    self.unify_heads = nn.Linear(k * heads, k)\n",
    "\n",
    "    self.attention = DotProductAttention(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Implements forward pass of self-attention layer\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Batch x t x k sized input\n",
    "\n",
    "    Returns:\n",
    "      unify_heads: Tensor\n",
    "        Self-attention based unified Query/Value/Key tensors\n",
    "    \"\"\"\n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "\n",
    "    # We reshape the queries, keys and values so that each head has its own dimension\n",
    "    queries = self.to_queries(x).view(b, t, h, k)\n",
    "    keys = self.to_keys(x).view(b, t, h, k)\n",
    "    values = self.to_values(x).view(b, t, h, k)\n",
    "\n",
    "    out = self.attention(queries, keys, values, b, h, t, k)\n",
    "\n",
    "    return self.unify_heads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47032719",
   "metadata": {},
   "source": [
    "In practice PyTorchs torch.nn.MultiheadAttention() function is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cd06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  \"\"\" Transformer Encoder network for classification. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "    \"\"\"\n",
    "    Initiates the Transformer Network\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Attention embedding size\n",
    "      heads: Integer\n",
    "        Number of self attention heads\n",
    "      depth: Integer\n",
    "        Number of Transformer Blocks\n",
    "      seq_length: Integer\n",
    "        Length of input sequence\n",
    "      num_tokens: Integer\n",
    "        Size of dictionary\n",
    "      num_classes: Integer\n",
    "        Number of output classes\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.k = k\n",
    "    self.num_tokens = num_tokens\n",
    "    self.token_embedding = nn.Embedding(num_tokens, k)\n",
    "    self.pos_enc = PositionalEncoding(k)\n",
    "\n",
    "    transformer_blocks = []\n",
    "    for i in range(depth):\n",
    "      transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n",
    "\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.classification_head = nn.Linear(k, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass for Classification within Transformer network\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        (b, t) sized tensor of tokenized words\n",
    "\n",
    "    Returns:\n",
    "      logprobs: Tensor\n",
    "        Log-probabilities over classes sized (b, c)\n",
    "    \"\"\"\n",
    "    x = self.token_embedding(x) * np.sqrt(self.k)\n",
    "    x = self.pos_enc(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "\n",
    "    sequence_avg = x.mean(dim=1)\n",
    "    x = self.classification_head(sequence_avg)\n",
    "    logprobs = F.log_softmax(x, dim=1)\n",
    "    return logprobs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
