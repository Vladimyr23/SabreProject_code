{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24599f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#Environment set-up and libraries\n",
    "\n",
    "#Base libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "#Plotting libraries\n",
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#Utilities libraries\n",
    "from glob import glob \n",
    "import os\n",
    "\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae60c9",
   "metadata": {},
   "source": [
    "### Loading the point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26404047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    print(file_name)\n",
    "\n",
    "    if file_name.endswith(\".las\") or file_name.endswith(\".laz\"):\n",
    "        print(\"[INFO] .las (.laz) file loading\")\n",
    "        try:\n",
    "            # import lidar .las data and assign to variable\n",
    "            pcd = laspy.read(file_name)\n",
    "            # examine the available features for the lidar file we have read\n",
    "            # list(las.point_format.dimension_names)\n",
    "            #\n",
    "            # set(list(las.classification))\n",
    "\n",
    "            # Creating, Filtering, and Writing Point Cloud Data\n",
    "            # To create 3D point cloud data, we can stack together with the X, Y, and Z dimensions, using Numpy like this.\n",
    "            point_data = np.stack([pcd.X, pcd.Y, pcd.Z], axis=0).transpose((1, 0))\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_data)\n",
    "            # points = point_data\n",
    "            if pcd is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "\n",
    "                # Point cloud\n",
    "                return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".las, .laz file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".e57\"):\n",
    "        print(\"[INFO] .e57 file loading\")\n",
    "        try:\n",
    "            e57_file = pye57.E57(file_name)\n",
    "\n",
    "            # other attributes can be read using:\n",
    "            data = e57_file.read_scan(0)\n",
    "\n",
    "            # 'data' is a dictionary with the point types as keys\n",
    "            # assert isinstance(data[\"cartesianX\"], np.ndarray)\n",
    "            # assert isinstance(data[\"cartesianY\"], np.ndarray)\n",
    "            # assert isinstance(data[\"cartesianZ\"], np.ndarray)\n",
    "\n",
    "            point_xyz = np.stack([data[\"cartesianX\"], data[\"cartesianY\"], data[\"cartesianZ\"]]).transpose((1, 0))\n",
    "            # points_rgb = [data[\"colorRed\"], data[\"colorGreen\"], data[\"colorBlue\"]]\n",
    "            # points_intensity = data[\"intensity\"]\n",
    "\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            # points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            # points = point_xyz\n",
    "            # pcd.colors = o3d.utility.Vector3dVector(points_rgb)\n",
    "            # pcd.colors[:, 0] = points_intensity\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".e57 file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".bin\"):\n",
    "        print(\"[INFO] .bin file loading\")\n",
    "        try:\n",
    "            size_float = 4\n",
    "            list_pcd = []\n",
    "            with open(file_name, \"rb\") as f:\n",
    "                byte = f.read(size_float * 4)\n",
    "                while byte:\n",
    "                    x, y, z, intensity = struct.unpack(\"ffff\", byte)\n",
    "                    list_pcd.append([x, y, z])\n",
    "                    byte = f.read(size_float * 4)\n",
    "            np_pcd = np.asarray(list_pcd)\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(np_pcd)\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\".bin file load failed\")\n",
    "\n",
    "    elif file_name.endswith(\".ply\"):\n",
    "        pcd = o3d.io.read_point_cloud(file_name)\n",
    "        points_xyz = np.asarray(pcd.points)\n",
    "        #pcd = o3d.geometry.PointCloud() # No need to do that already a PointCloud\n",
    "        pcd.points = o3d.utility.Vector3dVector(points_xyz)\n",
    "        # points = points_xyz\n",
    "        if pcd is not None:\n",
    "            print(\"[Info] Successfully read\", file_name)\n",
    "            # Point cloud\n",
    "            return pcd\n",
    "\n",
    "    elif file_name.endswith(\".pts\"):\n",
    "        try:\n",
    "            with open(file_name, \"r\") as f:\n",
    "                # Log every 1000000 lines.\n",
    "                LOG_EVERY_N = 1000000\n",
    "                points_np = []\n",
    "                for line in f:\n",
    "                    if len(line.split()) == 4:\n",
    "                        x, y, z, i = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z), float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 3:\n",
    "                        x, y, z = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 5:\n",
    "                        x, y, z, i, zeroes_v = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z), float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    elif len(line.split()) == 7:\n",
    "                        x, y, z, r, g, b, i = [num for num in line.split()]\n",
    "                        points_np.append([float(x), float(y), float(z),\n",
    "                                          float(r), float(g), float(b),\n",
    "                                          float(i)])\n",
    "                        if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "                            print('point', len(points_np))\n",
    "                    else:\n",
    "                        print(\"[Info] The file has unregistered format\")\n",
    "                        return\n",
    "            print('loop end')\n",
    "            points_arr = np.array(points_np).transpose()\n",
    "            print(len(points_arr))\n",
    "            point_xyz = points_arr[:3].transpose()\n",
    "            print(\"xyz points shape\", point_xyz.shape)\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "            if len(points_arr) == 4:\n",
    "                points_intensity = (points_arr[3])/255.0\n",
    "                print(\"intensity points len\", points_intensity.shape)\n",
    "                points_intensity_rgb = np.vstack((points_intensity,\n",
    "                                                  points_intensity,\n",
    "                                                  points_intensity)).T\n",
    "                print(\"intensity_rgb points shape\", points_intensity_rgb.shape)\n",
    "                pcd.colors = o3d.utility.Vector3dVector(points_intensity_rgb)\n",
    "            elif len(points_arr) == 7:\n",
    "                points_red = (points_arr[4]) / 255.0\n",
    "                points_green = (points_arr[5]) / 255.0\n",
    "                points_blue = (points_arr[6]) / 255.0\n",
    "                points_rgb = np.vstack((points_red,\n",
    "                                        points_green,\n",
    "                                        points_blue)).T\n",
    "\n",
    "                # points_intensity = ((points_arr[3]) / 255.0).T\n",
    "                # print(\"intensity points len\", points_intensity.shape)\n",
    "                print(\"rgb points shape\", points_rgb.shape)\n",
    "                pcd.colors = o3d.utility.Vector3dVector(points_rgb)\n",
    "                #pcd.intensities = o3d.utility.Vector3dVector(points_intensity)\n",
    "            if pcd is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "                # Point cloud\n",
    "                return pcd\n",
    "\n",
    "        except Exception:\n",
    "            print(\"[Info] Reading .pts file failed\", file_name)\n",
    "\n",
    "    # elif file_name.endswith(\".kml\"):\n",
    "    #     try:\n",
    "    #         with open(file_name, \"r\") as f:\n",
    "    #             # Log every 1000000 lines.\n",
    "    #             LOG_EVERY_N = 1000000\n",
    "    #             points_np = []\n",
    "    #             for line in f:\n",
    "    #                 print(line)\n",
    "    #                 if len(line.split(\",\")) == 3 and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "    #                     y, x, z = [num for num in line.split(\",\")]\n",
    "    #                     points_np.append([float(x), float(y), float(z)])\n",
    "    #                     if (len(points_np) % LOG_EVERY_N) == 0:\n",
    "    #                         print('point', len(points_np))\n",
    "    #                 else:\n",
    "    #                     print(\"[Info] The file has unregistered format\")\n",
    "    #         print('loop end')\n",
    "    #         points_arr = np.array(points_np).transpose()\n",
    "    #         print(len(points_arr))\n",
    "    #         point_xyz = points_arr[:3].transpose()\n",
    "    #         # points_intensity = points_arr[3]\n",
    "    #         pcd = o3d.geometry.PointCloud()\n",
    "    #         pcd.points = o3d.utility.Vector3dVector(point_xyz)\n",
    "    #         if pcd is not None:\n",
    "    #             print(\"[Info] Successfully read\", file_name)\n",
    "    #             # Point cloud\n",
    "    #             return pcd\n",
    "    #\n",
    "    #     except Exception:\n",
    "    #         print(\"[Info] Reading .kml file failed\", file_name)\n",
    "\n",
    "    else:\n",
    "        pcd = None\n",
    "        geometry_type = o3d.io.read_file_geometry_type(file_name)\n",
    "        print(geometry_type)\n",
    "\n",
    "        mesh = None\n",
    "        if geometry_type & o3d.io.CONTAINS_TRIANGLES:\n",
    "            mesh = o3d.io.read_triangle_model(file_name)\n",
    "        if mesh is None:\n",
    "            print(\"[Info]\", file_name, \"appears to be a point cloud\")\n",
    "            cloud = None\n",
    "            try:\n",
    "                cloud = o3d.io.read_point_cloud(file_name)\n",
    "                # print(type(cloud))\n",
    "            except Exception:\n",
    "                print(\"[Info] Unknown filename\", file_name)\n",
    "            if cloud is not None:\n",
    "                print(\"[Info] Successfully read\", file_name)\n",
    "\n",
    "                if not cloud.has_normals():\n",
    "                    cloud.estimate_normals()\n",
    "                cloud.normalize_normals()\n",
    "                pcd = cloud\n",
    "                #points = cloud.points\n",
    "                pcd.points = o3d.utility.Vector3dVector(cloud.points)\n",
    "            else:\n",
    "                print(\"[WARNING] Failed to read points\", file_name)\n",
    "\n",
    "        if pcd is not None or mesh is not None:\n",
    "            try:\n",
    "                if mesh is not None:\n",
    "                    # Triangle model\n",
    "                    _scene.scene.add_model(\"__model__\", mesh)\n",
    "                else:\n",
    "                    # Point cloud\n",
    "                    return pcd\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba714e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/SABRE MMS_S3 - 0002.pts\n",
      "point 1000000\n",
      "point 2000000\n",
      "point 3000000\n",
      "point 4000000\n",
      "point 5000000\n",
      "point 6000000\n",
      "point 7000000\n",
      "point 8000000\n",
      "point 9000000\n",
      "point 10000000\n",
      "point 11000000\n",
      "loop end\n",
      "4\n",
      "xyz points shape (11008825, 3)\n",
      "intensity points len (11008825,)\n",
      "intensity_rgb points shape (11008825, 3)\n",
      "[Info] Successfully read /home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/SABRE MMS_S3 - 0002.pts\n",
      "/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/SABRE Static Scan_T17_003.pts\n",
      "point 1000000\n",
      "point 2000000\n",
      "point 3000000\n",
      "point 4000000\n",
      "point 5000000\n",
      "point 6000000\n",
      "point 7000000\n",
      "point 8000000\n",
      "point 9000000\n",
      "point 10000000\n",
      "point 11000000\n",
      "point 12000000\n",
      "point 13000000\n",
      "point 14000000\n",
      "point 15000000\n",
      "point 16000000\n",
      "point 17000000\n",
      "loop end\n",
      "4\n",
      "xyz points shape (17814760, 3)\n",
      "intensity points len (17814760,)\n",
      "intensity_rgb points shape (17814760, 3)\n",
      "[Info] Successfully read /home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/SABRE Static Scan_T17_003.pts\n"
     ]
    }
   ],
   "source": [
    "filepath_mob1 = \"/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE ADVANCED 3D - Selected MMS Data/\"\n",
    "filename1 = \"SABRE MMS_S3 - 0002.pts\"\n",
    "filepath1 = filepath_mob1 + filename1\n",
    "pc1 = load_file(filepath1)\n",
    "\n",
    "filepath_static2 = \"/home/mekala/PycharmProjects/SabreProject_code/Sabre_proj/SABRE - Selected Static Scan Data/SABRE - Selected Static Scan Data/\"\n",
    "filename2 = \"SABRE Static Scan_T17_003.pts\"\n",
    "filepath2 = filepath_static2 + filename2\n",
    "pc2 = load_file(filepath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a747c",
   "metadata": {},
   "source": [
    "#### pc1 and pc2 are the original size point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec64983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function that returns the down sampled point cloud \n",
    "and fpfh parameters of the down sampled point cloud\n",
    "the down sampling defined by the parameter voxel_size\n",
    "'''\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2.0,\n",
    "                                             max_nn=30))\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5.0,\n",
    "                                             max_nn=100))\n",
    "    return pcd_down, pcd_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90c7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "voxel_size = 0.3\n",
    "\n",
    "pc1_down, pc1_fpfh = preprocess_point_cloud(pc1, voxel_size)\n",
    "pc2_down, pc2_fpfh = preprocess_point_cloud(pc2, voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb08987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56746, 3)\n",
      "(123234, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sizes of the downsampled point clouds points sets\n",
    "pc1_down_points = np.asarray(pc1_down.points)\n",
    "pc2_down_points = np.asarray(pc2_down.points)\n",
    "print(pc1_down_points.shape)\n",
    "print(pc2_down_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed99bad",
   "metadata": {},
   "source": [
    "#### Defining device for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99342f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde03c0",
   "metadata": {},
   "source": [
    "### Initial RANSAC alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7806ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voxel_size =  0.3\n",
      "Distance threshold:  0.75\n",
      "mutual_filter =  True\n",
      "max_iterations =  1000000\n",
      "max_validation =  28373\n",
      "\n",
      "RANSAC Started 10/24/2023, 13:08:58 \n",
      "\n",
      "Running RANSAC\n",
      "\n",
      "RANSAC Finished 10/24/2023, 13:09:03 \n",
      "Global registration took 4.783 sec.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"voxel_size = \", voxel_size)\n",
    "distance_threshold = 2.5 * voxel_size\n",
    "print(\"Distance threshold: \", distance_threshold)\n",
    "mutual_filter = True\n",
    "print(\"mutual_filter = \", mutual_filter)\n",
    "max_iterations = 1000000\n",
    "print(\"max_iterations = \", max_iterations)\n",
    "max_validation = np.min([len(pc1_down.points), len(pc2_down.points)]) // 2\n",
    "print(\"max_validation = \", max_validation)\n",
    "\n",
    "# getting the current date and time\n",
    "start = datetime.now()\n",
    "# getting the date and time from the current date and time in the given format\n",
    "start_date_time = start.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print('\\nRANSAC Started', start_date_time, '\\n')\n",
    "print('Running RANSAC\\n')\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    pc1_down, pc2_down, pc1_fpfh, pc2_fpfh,\n",
    "    mutual_filter=mutual_filter,\n",
    "    max_correspondence_distance=distance_threshold,\n",
    "    estimation_method=o3d.pipelines.registration.\n",
    "    TransformationEstimationPointToPoint(True),\n",
    "    ransac_n=3,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(\n",
    "        max_iterations, max_validation))  # max_validation replaces args.confidence in mobile-static\n",
    "# getting the current date and time\n",
    "finish = datetime.now()\n",
    "# getting the date and time from the current date and time in the given format\n",
    "finish_date_time = finish.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print('RANSAC Finished', finish_date_time,\n",
    "      \"\\nGlobal registration took %.3f sec.\\n\" % (finish - start).total_seconds())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e74e37",
   "metadata": {},
   "source": [
    "#### RANSAC transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e1824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated transformation matrix:\n",
      "[[-9.03772418e-01 -1.08568484e-01  6.37057409e-03  1.53870607e+06]\n",
      " [ 1.08726265e-01 -9.03210808e-01  3.19549352e-02  6.45150100e+06]\n",
      " [ 2.50982261e-03  3.24869649e-02  9.09709064e-01 -2.06839037e+05]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Saving the transformation matrix in ransac_transformation_matrix.txt ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trans = result.transformation\n",
    "print(\"The estimated transformation matrix:\")\n",
    "print(trans)\n",
    "print(\"Saving the transformation matrix in ransac_transformation_matrix.txt ...\")\n",
    "np.savetxt('ransac_transformation_matrix.txt', trans)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea63248",
   "metadata": {},
   "source": [
    "#### Applying RANSAC transformation on original and downsampled point clouds and visualising the result with original point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41470371",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_down_ransac = pc1_down.transform(result.transformation)\n",
    "pc1_ransac = pc1.transform(result.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5281824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56746, 3)\n",
      "(123234, 3)\n"
     ]
    }
   ],
   "source": [
    "pc1_down_ransac_points = np.asarray(pc1_down_ransac.points)\n",
    "print(pc1_down_ransac_points.shape)\n",
    "print(pc2_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f20a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 17814760 points."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coloring the point clouds\n",
    "source_color=(1, 0.706, 0)\n",
    "target_color=(0, 0.651, 0.929)\n",
    "pc1_ransac.paint_uniform_color(source_color)\n",
    "pc2.paint_uniform_color(target_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0f585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pc1_ransac, pc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602d476",
   "metadata": {},
   "source": [
    "#### Cropping downsampled point cloud 2 (static scan) to the size of downsampled and transformed point cloud 1 (mobile scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f377c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop point cloud 2 to the size of transformed point cloud 1\n",
    "oriented_bounding_box = pc1_down_ransac.get_oriented_bounding_box()\n",
    "oriented_bounding_box.color = (0, 1, 0)\n",
    "pc2_down_croppped = pc2_down.crop(oriented_bounding_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6338820c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointCloud with 86900 points."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coloring the downsampled, traansformed point cloud 1 \n",
    "# and the cropped point cloud 2\n",
    "pc1_down_ransac.paint_uniform_color(source_color)\n",
    "pc2_down_croppped.paint_uniform_color(target_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd313f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both point clouds with visualization of the bbox\n",
    "o3d.visualization.draw_geometries([pc1_down_ransac, pc2_down_croppped, oriented_bounding_box, pc1_down_ransac])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8977971",
   "metadata": {},
   "source": [
    "### Chamfer distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bde414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chamfer_distance(pcd1, pcd2):\n",
    "    \"\"\"\n",
    "    Compute the Chamfer distance between two point clouds.\n",
    "\n",
    "    Parameters:\n",
    "    - pcd1, pcd2: Open3D point cloud objects.\n",
    "\n",
    "    Returns:\n",
    "    - chamfer_distance: The Chamfer distance between the two point clouds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute distance from pcd1 to pcd2\n",
    "    distances_1_to_2 = pcd1.compute_point_cloud_distance(pcd2)\n",
    "    avg_distance_1_to_2 = np.mean([np.min(dist) for dist in distances_1_to_2])\n",
    "\n",
    "    # Compute distance from pcd2 to pcd1\n",
    "    distances_2_to_1 = pcd2.compute_point_cloud_distance(pcd1)\n",
    "    avg_distance_2_to_1 = np.mean([np.min(dist) for dist in distances_2_to_1])\n",
    "\n",
    "    # Compute the Chamfer distance\n",
    "    chamfer_distance = (avg_distance_1_to_2 + avg_distance_2_to_1) / 2\n",
    "\n",
    "    return chamfer_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb876bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer Distance: 2.885246918664532\n"
     ]
    }
   ],
   "source": [
    "chamfer_dist = compute_chamfer_distance(pc1_down_ransac, pc2_down_croppped)\n",
    "print(f\"Chamfer Distance: {chamfer_dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c567d",
   "metadata": {},
   "source": [
    "#### RANSAC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd5daeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness:\n",
      "0.682109752229232\n",
      "\n",
      "RMSE of all inlier correspondences:\n",
      "0.33023046201882267\n",
      "\n",
      "Correspondence Set:\n",
      "std::vector<Eigen::Vector2i> with 38707 elements.\n",
      "Use numpy.asarray() to access data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RANSAC Evaluation\n",
    "\n",
    "fitness = result.fitness\n",
    "print(\"Fitness:\")\n",
    "print(fitness)\n",
    "print(\"\")\n",
    "\n",
    "rmse = result.inlier_rmse\n",
    "print(\"RMSE of all inlier correspondences:\")\n",
    "print(rmse)\n",
    "print(\"\")\n",
    "\n",
    "# trans = result.transformation\n",
    "# print(\"The estimated transformation matrix:\")\n",
    "# print(trans)\n",
    "# print(\"Saving the transformation matrix in ransac_transformation_matrix.txt ...\")\n",
    "# np.savetxt('ransac_transformation_matrix.txt', trans)\n",
    "# print(\"\")\n",
    "\n",
    "correspondences = result.correspondence_set\n",
    "print(\"Correspondence Set:\")\n",
    "print(correspondences)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c7a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def registration_error(sour, targ):\n",
    "    # # Make source and target of the same size\n",
    "    # minimum_len = min(len(sour), len(targ))\n",
    "    # source = sour[:minimum_len, :3]\n",
    "    # target = sour[:minimum_len, :3]\n",
    "    # # Apply transformation to point cloud\n",
    "    # source_transformed = np.dot(transformation[:3, :3], source.T).T + transformation[:3, 3]\n",
    "    # # Compute the difference between the transformed source and target point clouds\n",
    "    # diff = np.subtract(target, source_transformed)\n",
    "    # # RMSE of the difference\n",
    "    # rmse = np.sqrt(np.mean(np.sum(diff ** 2, axis=1)))\n",
    "    # # Compute the rotational error using quaternions\n",
    "    # r = R.from_matrix(transformation)\n",
    "    # q = r.as_quat()\n",
    "    # q_target = R.from_matrix(np.identity(3)).as_quat()\n",
    "    # rot_error = np.arccos(np.abs(np.dot(q, q_target))) * 180 / np.pi\n",
    "    # # Compute the translational error\n",
    "    # trans_error = np.linalg.norm(transformation - np.array([0, 0, 0]))\n",
    "    # return rmse, rot_error, trans_error\n",
    "    print('Calculating errors...')\n",
    "    # Calculate the centroid of the source and target points\n",
    "    source_centroid = np.mean(sour, axis=0)\n",
    "    target_centroid = np.mean(targ, axis=0)\n",
    "    print(f'Sour centroid: {source_centroid}')\n",
    "    print(f'Targ centroid: {target_centroid}')\n",
    "\n",
    "    # Calculate the covariance matrix of the source and target points\n",
    "    source_covariance = np.cov(sour.T)\n",
    "    target_covariance = np.cov(targ.T)\n",
    "\n",
    "    # Calculate the singular value decomposition of the covariance matrices\n",
    "    U_source, S_source, Vt_source = np.linalg.svd(source_covariance)\n",
    "    U_target, S_target, Vt_target = np.linalg.svd(target_covariance)\n",
    "\n",
    "    # Calculate the rotation matrix\n",
    "    rot = Vt_target.T @ U_source.T\n",
    "\n",
    "    # Calculate the translation vector\n",
    "    transl = target_centroid - rot @ source_centroid\n",
    "    print(f'Transl vector: {transl}')\n",
    "\n",
    "    rot_err = rot - np.eye(3)\n",
    "    # Mean Absolute error for each axis (row in rot_err)\n",
    "    rot_mae_xyz = np.mean(np.abs(rot_err), axis=1)\n",
    "\n",
    "    # Calculating translational error\n",
    "    transl_xyz = np.divide(np.abs(transl), (np.abs(source_centroid)+np.abs(target_centroid)+np.abs(transl))/3)\n",
    "    transl_xyz_mae = np.divide(transl_xyz, 100)\n",
    "    # Calculate the mean squared error\n",
    "    #mse = np.mean(np.sum((targ - (sour @ rot.T + transl)) ** 2, axis=1))\n",
    "\n",
    "    return rot_mae_xyz, transl_xyz_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7ab063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating errors...\n",
      "Sour centroid: [3.71361438e+05 7.96958461e+05 6.61827057e+01]\n",
      "Targ centroid: [3.71367689e+05 7.96933776e+05 7.01093962e+01]\n",
      "Transl vector: [-339858.66636905  280119.51815725   10662.76187682]\n",
      "Rotational MAE error xyz: [0.21920223 0.23031416 0.04467483], Translational MAE error xyz: [0.00941795 0.00448428 0.02962138]\n",
      "Rotational MAE: 0.1647304073260934, Translational MAE: 0.014507868523116326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We have pc1_down_ransac_points need pc2_down_croppped_points\n",
    "pc2_down_croppped_points = np.asarray(pc2_down_croppped.points)\n",
    "\n",
    "rot_err, transl_err = registration_error(pc1_down_ransac_points, pc2_down_croppped_points)\n",
    "print(f'Rotational MAE error xyz: {rot_err}, Translational MAE error xyz: {transl_err}')\n",
    "print(f'Rotational MAE: {np.mean(rot_err)}, Translational MAE: {np.mean(transl_err)}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c46cc9",
   "metadata": {},
   "source": [
    "## TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdccec",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af092d",
   "metadata": {},
   "source": [
    "#### Downsampling again to fit the memory with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc0d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess point cloud data. One more downsampling\n",
    "voxel_size = 0.8  # Adjust as needed\n",
    "source_pc_down, source_fpfh = preprocess_point_cloud(pc1_down_ransac, voxel_size)\n",
    "target_pc_down, target_fpfh = preprocess_point_cloud(pc2_down_croppped, voxel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b06d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7895, 3)\n",
      "(16549, 3)\n"
     ]
    }
   ],
   "source": [
    "# Downsampling result\n",
    "source_pc_down_points = np.asarray(source_pc_down.points)\n",
    "target_pc_down_points = np.asarray(target_pc_down.points)\n",
    "print(source_pc_down_points.shape)\n",
    "print(target_pc_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "340606b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([source_pc_down])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df0182",
   "metadata": {},
   "source": [
    "#### Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a00e917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17280\n",
      "54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the desired number of points for each batch \n",
    "# batch_size = len(batch_sizes) = 8\n",
    "batch_size = 54\n",
    "batch_sizes = [320]*batch_size  # Adjust as needed\n",
    "#batch_sizes[0] = 2048\n",
    "print(sum(batch_sizes))\n",
    "print(len(batch_sizes))\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3d773",
   "metadata": {},
   "source": [
    "#### The function below creates for each point cloud batch non overlapping batch with fpfh parameters batch. When a batch is smaller then a batch_size the function adds the padding. At the end it makes the cuda points and fpfh tensors of floats for the PyTorch Transformer input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cabefb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non overlaping batches\n",
    "\n",
    "def create_batches_with_padding(pcd, batch_sizes):\n",
    "    num_batches = len(batch_sizes)\n",
    "    batches_points = []\n",
    "    batches_fpfh = []\n",
    "    batch_start = 0\n",
    "    points = np.asarray(pcd.points)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_size = batch_sizes[i]\n",
    "        print('batch_size', batch_size)\n",
    "\n",
    "        # Initialize empty arrays for the current batch\n",
    "        batch_points = []\n",
    "        batch_fpfh = []\n",
    "        \n",
    "        # Cut the point cloud points to the size of the batch\n",
    "        if (len(points)-batch_start)>0:\n",
    "            batch_points = points[batch_start:(batch_start+batch_size)]\n",
    "        \n",
    "        # Calculate padding sizes\n",
    "        pad_points = batch_size - len(batch_points)\n",
    "        print('pad_points ', pad_points)\n",
    "\n",
    "        # Pad point cloud and FPFH to match the batch size\n",
    "        if len(batch_points)>0:\n",
    "            batch_points = np.pad(batch_points, [(0, pad_points), (0, 0)], mode='constant')\n",
    "\n",
    "\n",
    "            # FPFH for the points cut\n",
    "            batch_point_cloud = o3d.geometry.PointCloud()\n",
    "            batch_point_cloud.points = o3d.utility.Vector3dVector(batch_points)\n",
    "\n",
    "            batch_point_cloud.estimate_normals(\n",
    "                o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 2.0, max_nn=30))\n",
    "            fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "                batch_point_cloud, o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 5.0, max_nn=100))\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the batch to PyTorch tensors\n",
    "        batch_points = torch.FloatTensor(batch_points).cuda()#, dtype=torch.float32)\n",
    "        #batch_fpfh = torch.tensor(fpfh, dtype=torch.float32)\n",
    "        batch_fpfh = torch.FloatTensor(np.asarray(fpfh.data).copy()).T.cuda()\n",
    "\n",
    "        batches_points.append(batch_points)\n",
    "        batches_fpfh.append(batch_fpfh)\n",
    "        batch_start += batch_size\n",
    "\n",
    "    return batches_points, batches_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8019afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  105\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "sour_batches_points, sour_batches_fpfh = create_batches_with_padding(source_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "524d53ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(sour_batches_points))\n",
    "print(len(sour_batches_fpfh))\n",
    "print(sour_batches_points[1].shape)\n",
    "print(sour_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be14a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  91\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "targ_batches_points, targ_batches_fpfh = create_batches_with_padding(target_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f24cba61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "torch.Size([0])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(targ_batches_points))\n",
    "print(len(targ_batches_fpfh))\n",
    "print(targ_batches_points[53].shape)\n",
    "print(targ_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ca506",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c84c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b228296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targ_batches_points[51].cpu().detach().numpy() != [0,0,0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dd856f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_batches_points[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cd05a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_points = []\n",
    "target_test_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "for i in range(len(targ_batches_points)):\n",
    "    # Convert the aligned data to a NumPy array of shape (N, 3)\n",
    "    target_batch = targ_batches_points[i].cpu().detach().numpy()  # Assuming 'aligned_source' is a PyTorch tensor\n",
    "    target_batch_points = []\n",
    "    if len(target_batch)>0:\n",
    "        for point in target_batch:\n",
    "            if (point != [0,0,0]).all():\n",
    "                target_batch_points.append(point)\n",
    "    \n",
    "    if len(target_batch_points)>0:\n",
    "        # Store aligned data\n",
    "        target_test_points.append(target_batch_points)\n",
    "        # Create an Open3D point cloud and assign the aligned data\n",
    "        target_batch_pcd = o3d.geometry.PointCloud()\n",
    "        target_batch_pcd.points = o3d.utility.Vector3dVector(target_batch_points)\n",
    "        target_test_pcd += target_batch_pcd\n",
    "        o3d.visualization.draw_geometries([target_batch_pcd])\n",
    "    \n",
    "o3d.visualization.draw_geometries([target_test_pcd])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc292911",
   "metadata": {},
   "source": [
    "#### Overlaping batches with 20 points overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8e64a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7895, 3)\n",
      "(16549, 3)\n"
     ]
    }
   ],
   "source": [
    "print(source_pc_down_points.shape)\n",
    "print(target_pc_down_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fe8438f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320,\n",
       " 320]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the desired number of points for each batch \n",
    "# batch_size = len(batch_sizes) = 8\n",
    "batch_size = 58\n",
    "batch_sizes = [320]*batch_size  # Adjust as needed\n",
    "#batch_sizes[0] = 2048\n",
    "print(sum(batch_sizes)-20*batch_size)\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7aff88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaping batches\n",
    "\n",
    "def create_overlapping_batches_with_padding(pcd, batch_sizes):\n",
    "    num_batches = len(batch_sizes)\n",
    "    batches_points = []\n",
    "    batches_fpfh = []\n",
    "    batch_start = 0\n",
    "    points = np.asarray(pcd.points)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_size = batch_sizes[i]\n",
    "        print('batch_size', batch_size)\n",
    "\n",
    "        # Initialize empty arrays for the current batch\n",
    "        batch_points = []\n",
    "        batch_fpfh = []\n",
    "        \n",
    "        # Cut the point cloud points to the size of the batch\n",
    "        if (len(points)-batch_start)>0:\n",
    "            batch_points = points[batch_start:(batch_start+batch_size)]\n",
    "        \n",
    "        # Calculate padding sizes\n",
    "        pad_points = batch_size - len(batch_points)\n",
    "        print('pad_points ', pad_points)\n",
    "\n",
    "        # Pad point cloud and FPFH to match the batch size\n",
    "        if len(batch_points)>0:\n",
    "            batch_points = np.pad(batch_points, [(0, pad_points), (0, 0)], mode='constant')\n",
    "\n",
    "\n",
    "            # FPFH for the points cut\n",
    "            batch_point_cloud = o3d.geometry.PointCloud()\n",
    "            batch_point_cloud.points = o3d.utility.Vector3dVector(batch_points)\n",
    "\n",
    "            batch_point_cloud.estimate_normals(\n",
    "                o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 2.0, max_nn=30))\n",
    "            fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "                batch_point_cloud, o3d.geometry.KDTreeSearchParamHybrid(\n",
    "                    radius=voxel_size * 5.0, max_nn=100))\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the batch to PyTorch tensors\n",
    "        batch_points = torch.FloatTensor(batch_points).cuda()#, dtype=torch.float32)\n",
    "        #batch_fpfh = torch.tensor(fpfh, dtype=torch.float32)\n",
    "        batch_fpfh = torch.FloatTensor(np.asarray(fpfh.data).copy()).T.cuda()\n",
    "\n",
    "        batches_points.append(batch_points)\n",
    "        batches_fpfh.append(batch_fpfh)\n",
    "        batch_start += (batch_size - 20)\n",
    "\n",
    "    return batches_points, batches_fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd376329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  225\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "sour_over_batches_points, sour_over_batches_fpfh = create_overlapping_batches_with_padding(source_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32480ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "58\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(sour_over_batches_points))\n",
    "print(len(sour_over_batches_fpfh))\n",
    "print(sour_over_batches_points[1].shape)\n",
    "print(sour_over_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ae26a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  0\n",
      "batch_size 320\n",
      "pad_points  271\n",
      "batch_size 320\n",
      "pad_points  320\n",
      "batch_size 320\n",
      "pad_points  320\n"
     ]
    }
   ],
   "source": [
    "targ_over_batches_points, targ_over_batches_fpfh = create_overlapping_batches_with_padding(target_pc_down, batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0452248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "58\n",
      "torch.Size([320, 3])\n",
      "torch.Size([320, 33])\n"
     ]
    }
   ],
   "source": [
    "print(len(targ_over_batches_points))\n",
    "print(len(targ_over_batches_fpfh))\n",
    "print(targ_over_batches_points[1].shape)\n",
    "print(targ_over_batches_fpfh[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763408db",
   "metadata": {},
   "source": [
    "#### Transformer result loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c0867059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PointCloudTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, d_model):\n",
    "        super(PointCloudTransformer, self).__init__()\n",
    "\n",
    "        # Embedding layers for point cloud points\n",
    "#         self.fpfh_embedding = nn.Embedding(input_dim, d_model)\n",
    "        #self.flattened_tensor = nn.Flatten()\n",
    "        self.point_embedding = nn.Linear(d_model, d_model)\n",
    "        self.point_embedding.weight.data = nn.init.xavier_uniform_(self.point_embedding.weight.data)  # Initialize weights\n",
    "        \n",
    "        # Multi-Head Self-Attention and Cross-Attention layers\n",
    "        self.self_attention = nn.MultiheadAttention(d_model, num_heads)\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-Forward layers\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Stacking multiple transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([self.build_transformer_layer(input_dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def build_transformer_layer(self, d_model):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            self.self_attention,\n",
    "            nn.LayerNorm(d_model),\n",
    "            self.cross_attention,\n",
    "            nn.LayerNorm(d_model),\n",
    "            self.ffn\n",
    "        )\n",
    "    \n",
    "    def forward(self, source_points, target_points):\n",
    "        # Embedding source and target FPFH and points\n",
    "#         source_fpfh_embedded = self.fpfh_embedding(source_fpfh)\n",
    "        #source_points = self.flattened_tensor(source_points)\n",
    "        source_points_embedded = self.point_embedding(source_points)\n",
    "        \n",
    "#         target_fpfh_embedded = self.fpfh_embedding(target_fpfh)\n",
    "        #target_points = self.flattened_tensor(target_points)\n",
    "        target_points_embedded = self.point_embedding(target_points)\n",
    "        \n",
    "#         # Concatenate source and target embeddings\n",
    "#         source_embedding = source_fpfh_embedded + source_points_embedded\n",
    "#         target_embedding = target_fpfh_embedded + target_points_embedded\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            # Self-attention for source and target\n",
    "            source_points_embedded, _ = layer[1](source_points_embedded, source_points_embedded, source_points_embedded)\n",
    "            target_points_embedded, _ = layer[1](target_points_embedded, target_points_embedded, target_points_embedded)\n",
    "            \n",
    "            # Cross-attention between source and target\n",
    "            source_points_embedded, _ = layer[3](source_points_embedded, target_points_embedded, target_points_embedded)\n",
    "            target_points_embedded, _ = layer[3](target_points_embedded, source_points_embedded, source_points_embedded)\n",
    "            \n",
    "            # Feed-forward layer\n",
    "            source_points_embedded = layer[5](source_points_embedded)\n",
    "            target_points_embedded = layer[5](target_points_embedded)\n",
    "        \n",
    "        return source_points_embedded, target_points_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "16f674f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target batch 0 with torch.Size([320, 3])\n",
      "Target and source batch 0 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 1 with torch.Size([320, 3])\n",
      "Target and source batch 1 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 2 with torch.Size([320, 3])\n",
      "Target and source batch 2 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 3 with torch.Size([320, 3])\n",
      "Target and source batch 3 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 4 with torch.Size([320, 3])\n",
      "Target and source batch 4 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 5 with torch.Size([320, 3])\n",
      "Target and source batch 5 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 6 with torch.Size([320, 3])\n",
      "Target and source batch 6 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 7 with torch.Size([320, 3])\n",
      "Target and source batch 7 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 8 with torch.Size([320, 3])\n",
      "Target and source batch 8 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 9 with torch.Size([320, 3])\n",
      "Target and source batch 9 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 10 with torch.Size([320, 3])\n",
      "Target and source batch 10 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 11 with torch.Size([320, 3])\n",
      "Target and source batch 11 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 12 with torch.Size([320, 3])\n",
      "Target and source batch 12 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 13 with torch.Size([320, 3])\n",
      "Target and source batch 13 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 14 with torch.Size([320, 3])\n",
      "Target and source batch 14 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 15 with torch.Size([320, 3])\n",
      "Target and source batch 15 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 16 with torch.Size([320, 3])\n",
      "Target and source batch 16 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 17 with torch.Size([320, 3])\n",
      "Target and source batch 17 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 18 with torch.Size([320, 3])\n",
      "Target and source batch 18 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 19 with torch.Size([320, 3])\n",
      "Target and source batch 19 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 20 with torch.Size([320, 3])\n",
      "Target and source batch 20 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 21 with torch.Size([320, 3])\n",
      "Target and source batch 21 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 22 with torch.Size([320, 3])\n",
      "Target and source batch 22 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 23 with torch.Size([320, 3])\n",
      "Target and source batch 23 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 24 with torch.Size([320, 3])\n",
      "Target and source batch 24 with torch.Size([320, 3])\n",
      "aligned_source_batch_points.shape=\n",
      "(320, 3)\n",
      "aligned_target_batch_points.shape=\n",
      "(320, 3)\n",
      "Processing target batch 25 with torch.Size([320, 3])\n",
      "Target and source batch 25 with torch.Size([0])\n",
      "Processing target batch 26 with torch.Size([320, 3])\n",
      "Target and source batch 26 with torch.Size([0])\n",
      "Processing target batch 27 with torch.Size([320, 3])\n",
      "Target and source batch 27 with torch.Size([0])\n",
      "Processing target batch 28 with torch.Size([320, 3])\n",
      "Target and source batch 28 with torch.Size([0])\n",
      "Processing target batch 29 with torch.Size([320, 3])\n",
      "Target and source batch 29 with torch.Size([0])\n",
      "Processing target batch 30 with torch.Size([320, 3])\n",
      "Target and source batch 30 with torch.Size([0])\n",
      "Processing target batch 31 with torch.Size([320, 3])\n",
      "Target and source batch 31 with torch.Size([0])\n",
      "Processing target batch 32 with torch.Size([320, 3])\n",
      "Target and source batch 32 with torch.Size([0])\n",
      "Processing target batch 33 with torch.Size([320, 3])\n",
      "Target and source batch 33 with torch.Size([0])\n",
      "Processing target batch 34 with torch.Size([320, 3])\n",
      "Target and source batch 34 with torch.Size([0])\n",
      "Processing target batch 35 with torch.Size([320, 3])\n",
      "Target and source batch 35 with torch.Size([0])\n",
      "Processing target batch 36 with torch.Size([320, 3])\n",
      "Target and source batch 36 with torch.Size([0])\n",
      "Processing target batch 37 with torch.Size([320, 3])\n",
      "Target and source batch 37 with torch.Size([0])\n",
      "Processing target batch 38 with torch.Size([320, 3])\n",
      "Target and source batch 38 with torch.Size([0])\n",
      "Processing target batch 39 with torch.Size([320, 3])\n",
      "Target and source batch 39 with torch.Size([0])\n",
      "Processing target batch 40 with torch.Size([320, 3])\n",
      "Target and source batch 40 with torch.Size([0])\n",
      "Processing target batch 41 with torch.Size([320, 3])\n",
      "Target and source batch 41 with torch.Size([0])\n",
      "Processing target batch 42 with torch.Size([320, 3])\n",
      "Target and source batch 42 with torch.Size([0])\n",
      "Processing target batch 43 with torch.Size([320, 3])\n",
      "Target and source batch 43 with torch.Size([0])\n",
      "Processing target batch 44 with torch.Size([320, 3])\n",
      "Target and source batch 44 with torch.Size([0])\n",
      "Processing target batch 45 with torch.Size([320, 3])\n",
      "Target and source batch 45 with torch.Size([0])\n",
      "Processing target batch 46 with torch.Size([320, 3])\n",
      "Target and source batch 46 with torch.Size([0])\n",
      "Processing target batch 47 with torch.Size([320, 3])\n",
      "Target and source batch 47 with torch.Size([0])\n",
      "Processing target batch 48 with torch.Size([320, 3])\n",
      "Target and source batch 48 with torch.Size([0])\n",
      "Processing target batch 49 with torch.Size([320, 3])\n",
      "Target and source batch 49 with torch.Size([0])\n",
      "Processing target batch 50 with torch.Size([320, 3])\n",
      "Target and source batch 50 with torch.Size([0])\n",
      "Processing target batch 51 with torch.Size([320, 3])\n",
      "Target and source batch 51 with torch.Size([0])\n",
      "Processing target batch 52 with torch.Size([0])\n",
      "Target and source batch 52 with torch.Size([0])\n",
      "Processing target batch 53 with torch.Size([0])\n",
      "Target and source batch 53 with torch.Size([0])\n",
      "Iteration 0/10: Loss = 6.092719218031561e-08\n",
      "(8000, 3)\n",
      "(8000, 3)\n",
      "Calculating errors...\n",
      "Sour centroid: [-0.23268058 -0.03625399  0.24340978]\n",
      "Targ centroid: [-0.23225772 -0.03631842  0.24342385]\n",
      "Transl vector: [-0.01674498 -0.02110231 -0.01739343]\n",
      "Rotational MAE error xyz: [0.06250532 0.03899624 0.02816645], \n",
      "Translational MAE error xyz: [0.0010429  0.00675817 0.00103486]\n",
      "Rotational MAE: 0.04322266703737976, \n",
      "Translational MAE: 0.0029453092615865334\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "draw_geometries(): incompatible function arguments. The following argument types are supported:\n    1. (geometry_list: List[open3d.cuda.pybind.geometry.Geometry], window_name: str = 'Open3D', width: int = 1920, height: int = 1080, left: int = 50, top: int = 50, point_show_normal: bool = False, mesh_show_wireframe: bool = False, mesh_show_back_face: bool = False) -> None\n    2. (geometry_list: List[open3d.cuda.pybind.geometry.Geometry], window_name: str = 'Open3D', width: int = 1920, height: int = 1080, left: int = 50, top: int = 50, point_show_normal: bool = False, mesh_show_wireframe: bool = False, mesh_show_back_face: bool = False, lookat: numpy.ndarray[numpy.float64[3, 1]], up: numpy.ndarray[numpy.float64[3, 1]], front: numpy.ndarray[numpy.float64[3, 1]], zoom: float) -> None\n\nInvoked with: [PointCloud with 8000 points.], [PointCloud with 8000 points.]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 115\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# The 'source_embedding' now represents the aligned source point cloud\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# You can convert it to a NumPy array for further processing\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43maligned_source_point_cloud\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43maligned_target_point_cloud\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: draw_geometries(): incompatible function arguments. The following argument types are supported:\n    1. (geometry_list: List[open3d.cuda.pybind.geometry.Geometry], window_name: str = 'Open3D', width: int = 1920, height: int = 1080, left: int = 50, top: int = 50, point_show_normal: bool = False, mesh_show_wireframe: bool = False, mesh_show_back_face: bool = False) -> None\n    2. (geometry_list: List[open3d.cuda.pybind.geometry.Geometry], window_name: str = 'Open3D', width: int = 1920, height: int = 1080, left: int = 50, top: int = 50, point_show_normal: bool = False, mesh_show_wireframe: bool = False, mesh_show_back_face: bool = False, lookat: numpy.ndarray[numpy.float64[3, 1]], up: numpy.ndarray[numpy.float64[3, 1]], front: numpy.ndarray[numpy.float64[3, 1]], zoom: float) -> None\n\nInvoked with: [PointCloud with 8000 points.], [PointCloud with 8000 points.]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Define the PointCloudTransformer class (use the previously provided code)\n",
    "\n",
    "# Example usage\n",
    "input_dim = 320  # Replace with the actual input dimension of the batch size *2\n",
    "num_heads = 1 # input_dim should be divisible by num_heads\n",
    "num_layers = 6 #increase the number of layers if the model needs more capacity.\n",
    "d_model = 3 # Number of dimensions points + fpfh\n",
    "hidden_dim = d_model*2 # common choice is d_model/2 or d_model*2\n",
    "\n",
    "# d_model_fpfh = 33 # fpfh dimensions\n",
    "# batch_size = 32\n",
    "# sequence_length = 10\n",
    "\n",
    "\n",
    "# Create the transformer model\n",
    "transformer = PointCloudTransformer(input_dim, num_heads, num_layers, hidden_dim, d_model).cuda()\n",
    "\n",
    "# Define a loss function and optimizer for registration\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Number of iterations (epochs) for registration\n",
    "num_iterations = 10\n",
    "losses=[]\n",
    "aligned_source_pcd_points = []\n",
    "aligned_target_pcd_points = []\n",
    "results = []\n",
    "aligned_source_point_cloud = o3d.geometry.PointCloud()\n",
    "aligned_target_point_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    for i in range(len(targ_batches_points)):\n",
    "        target_points = targ_batches_points[i].cuda()\n",
    "        #print(target_points)\n",
    "        \n",
    "#         target_fpfh = targ_batches_fpfh[i]\n",
    "        print(f'Processing target batch {i} with {target_points.shape}')\n",
    "\n",
    "#         model = Transformer(d_model=d_model, nhead=num_heads).cuda()\n",
    "        source_points = sour_batches_points[i].cuda()\n",
    "        #print(source_points)\n",
    "#         source_fpfh = sour_batches_fpfh[i]\n",
    "        print(f'Target and source batch {i} with {source_points.shape}')\n",
    "\n",
    "        if len(target_points)>0 and len(source_points)>0:\n",
    "            # Pass data through the transformer\n",
    "            source_embedding, target_embedding = transformer(source_points, target_points)\n",
    "\n",
    "            # Find nearest neighbors in the source point cloud for each point in the target point cloud\n",
    "            source_points_numpy = source_embedding.cpu().detach().numpy()\n",
    "            target_points_numpy = target_embedding.cpu().detach().numpy()\n",
    "            nn_model = NearestNeighbors(n_neighbors=1).fit(source_points_numpy)\n",
    "            distances, source_indices = nn_model.kneighbors(target_points_numpy)\n",
    "\n",
    "            aligned_source_batch_points = []\n",
    "            if len(source_points_numpy)>0:\n",
    "                for point in source_points_numpy:\n",
    "                    if (point != [0,0,0]).all():\n",
    "                        aligned_source_batch_points.append(point)\n",
    "                        \n",
    "            aligned_target_batch_points = []\n",
    "            if len(target_points_numpy)>0:\n",
    "                for point in target_points_numpy:\n",
    "                    if (point != [0,0,0]).all():\n",
    "                        aligned_target_batch_points.append(point)\n",
    "\n",
    "            if len(aligned_source_batch_points)>0:\n",
    "                print(f'aligned_source_batch_points.shape=\\n{np.asarray(aligned_source_batch_points).shape}')\n",
    "                # Store aligned data\n",
    "                aligned_source_pcd_points.extend(np.asarray(aligned_source_batch_points))\n",
    "                # Create an Open3D point cloud and assign the aligned data\n",
    "                aligned_source_batch_pcd = o3d.geometry.PointCloud()\n",
    "                aligned_source_batch_pcd.points = o3d.utility.Vector3dVector(aligned_source_batch_points)\n",
    "                aligned_source_point_cloud += aligned_source_batch_pcd\n",
    "                \n",
    "            \n",
    "            if len(aligned_target_batch_points)>0:\n",
    "                print(f'aligned_target_batch_points.shape=\\n{np.asarray(aligned_target_batch_points).shape}')\n",
    "                # Store aligned data\n",
    "                aligned_target_pcd_points.extend(np.asarray(aligned_target_batch_points))\n",
    "                # Create an Open3D point cloud and assign the aligned data\n",
    "                aligned_target_batch_pcd = o3d.geometry.PointCloud()\n",
    "                aligned_target_batch_pcd.points = o3d.utility.Vector3dVector(aligned_target_batch_points)\n",
    "                aligned_target_point_cloud += aligned_target_batch_pcd\n",
    "\n",
    "            # Calculate the loss as the mean squared error between aligned source and target points\n",
    "            loss = criterion(source_embedding, target_embedding)\n",
    "\n",
    "            # Zero the gradients, perform backpropagation, and update the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f\"Iteration {iteration}/{num_iterations}: Loss = {loss.item()}\")\n",
    "    losses.append(loss.item())\n",
    "#     print(aligned_source_point_cloud)\n",
    "#     print(aligned_target_point_cloud)\n",
    "    print(np.asarray(aligned_source_point_cloud.points).shape)\n",
    "    print(np.asarray(aligned_target_point_cloud.points).shape)\n",
    "    \n",
    "    rot_err, transl_err = registration_error(np.asarray(aligned_source_point_cloud.points), np.asarray(aligned_target_point_cloud.points))\n",
    "    print(f'Rotational MAE error xyz: {rot_err}, \\nTranslational MAE error xyz: {transl_err}')\n",
    "    print(f'Rotational MAE: {np.mean(rot_err)}, \\nTranslational MAE: {np.mean(transl_err)}')\n",
    "    print(\"\")\n",
    "\n",
    "    # The 'source_embedding' now represents the aligned source point cloud\n",
    "    # You can convert it to a NumPy array for further processing\n",
    "    \n",
    "    o3d.visualization.draw_geometries([aligned_source_point_cloud], [aligned_target_point_cloud])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623216d",
   "metadata": {},
   "source": [
    "Input Dimension (input_dim): Since you have FPFH features and point cloud points for both source and target inputs, the input_dim for your embedding layers should account for the total number of unique indices across all features. The input_dim should be large enough to cover the maximum index value across both FPFH and point cloud points. If the maximum index value for FPFH is 320 and for point cloud points is 320 as well, you might set input_dim to 640 to ensure coverage.\n",
    "\n",
    "Model Dimension (d_model): The d_model parameter determines the dimension of the embedding vectors. You can experiment with different values, but common choices include 128, 256, or 512. For example, setting d_model to 128 would result in 128-dimensional embeddings.\n",
    "\n",
    "Number of Layers (num_layers): The number of layers in the transformer can impact its capacity to learn from the data. Starting with a moderate number of layers (e.g., 6) is a reasonable choice, and you can adjust it based on the complexity of your registration task. You can increase the number of layers if the model needs more capacity.\n",
    "\n",
    "Number of Attention Heads (num_heads): The number of attention heads in the multi-head attention mechanism can be a hyperparameter to tune. A common choice is 4, but you can experiment with different values to see what works best for your data and task.\n",
    "\n",
    "Hidden Dimension (hidden_dim): The hidden dimension in the feed-forward layers can be set to a fraction of d_model. A common choice is d_model/2 or d_model*2. For example, if d_model is 128, you might set hidden_dim to 64 or 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c4d0887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 20)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9c585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
